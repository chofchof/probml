# 10. Logistic Regression

Logistic regression is a **discriminative classification** model $p(y|x;\theta)$, where $x\in\mathbb{R}^D$ is a input vector, $y\in\{1,\dotsc,C\}$ is the class label, and $\theta$ are the parameters.




## 10.2 Binary logistic regression

$$
p(y|x;\theta) = \operatorname{Ber}(y|\sigma(w^Tx+b)) \tag{10.1}
$$
where $\sigma$ is the **sigmoid** or **logistic** function and $\theta=(w,b)$. In other words,
$$
p(y=1|x;\theta) = \sigma(a) \equiv \frac{1}{1+e^{-a}} \tag{10.2}
$$
where $a=w^Tx+b=\log(\frac{p}{1-p})$ is the **log-odds** (see, eq. 2.83), or called the **logit** or the **pre-activation**.

If we choose the labels $\tilde y\in\{\pm1\}$ instead of $y\in\{0,1\}$, we can compute
$$
p(\tilde y|x;\theta) = \sigma(\tilde ya) \tag{10.3}
$$
since $\sigma(-a)=1-\sigma(a)$.



### 10.2.1 Linear classifiers

Since $p(y=0|x;\theta)=1-\sigma(a)=\sigma(-a)$ and $\dfrac{\sigma(a)}{\sigma(-a)}=e^a$, we have
$$
\mathbb{I}\bigl[p(y=1|x) > p(y=0|x)\bigr] = \mathbb{I}\Bigl[\frac{\sigma(a)}{1-\sigma(a)}>1\Bigr] = \mathbb{I}\Bigl[\log\frac{\sigma(a)}{\sigma(-a)}>0\Bigr] = \mathbb{I}(a>0) \tag{10.4}
$$
Thus the <u>prediction (or decision) function</u>
$$
f(x;\theta) = a = w^Tx + b \tag{10.5}
$$
defines a hyperplane (**decision boundary**) with normal vector $w\in\mathbb{R}^D$ and an offset $b\in\mathbb{R}$ from the origin. 

If we can perfectly separate the training examples by such a linear boundary, we say the data is **linearly separable**. (The data in Figure 10.1 below is not linearly separable. Here, $w=(5.75, 10.44)^T$ and $b=-45.27$.)

![](figure_10.1_b.png)

The vector $w$ defines the <u>orientation</u> of the decision boundary, and its magnitude $\|w\|$ controls the <u>steepness</u> of the sigmoid, and hence the <u>confidence of the predictions</u>.

![](steepness_of_sigmoid.png)



### 10.2.2 Nonlinear classifiers

We can often make a problem linearly separable by <u>preprocessing the inputs</u> in a suitable way.

For example, let $\phi(x_1,x_2)=(1,x_1^2,x_2^2)$ be a transformed version of the input feature vector $(x_1,x_2)$. If $w=(-R^2,1,1)$, then $w^T\phi(x_1,x_2)=x_1^2+x_2^2-R^2$, so the decision boundary is a circle with radius $R$.

![](https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_10.3.png)

We can gain even more power by <u>learning the parameters of the feature extractor</u> $\phi$ in addition to linear weights $w$. (See Part III. Deep Neural Networks.)



### 10.2.3 Maximum likelihood estimation

We discuss <u>how to estimate $w$ using MLE</u>. We assume that the bias term $b$ is absorbed into the weight vector $w$, i.e., we prepend each $x$ with $x_0=1$ and add $b=w_0$ to the first coordinate of $w$.



#### 10.2.3.1 Objective function (negative log likelihood)

$$
\begin{align*}
\operatorname{NLL}(w) &\equiv -\frac{1}{N}\log p(\mathcal{D}|w) = -\frac{1}{N}\log\prod_n\operatorname{Ber}(y_n|\sigma(a_n)) \tag{10.6} \\
&= -\frac{1}{N} \sum_n\bigl[y_n\log\sigma(a_n)+(1-y_n)\log(1-\sigma(a_n))\bigr] \tag{10.8} \\
&= \frac{1}{N} \sum_n\mathbb{H}(y_n,\sigma(a_n)) \tag{10.9}
\end{align*}
$$

where $a_n=w^Tx_n$ is the **logit** and $\mathbb{H}(p,q)\equiv -[p\log q+(1-p)\log(1-q)]$ is the **binary cross-entropy**.

If we use $\tilde y_n=\{\pm1\}$ instead of $y_n\in\{0,1\}$, then
$$
\begin{align*}
\operatorname{NLL}(w) &= -\frac{1}{N}\sum_n\Bigl[ \mathbb{I}(\tilde y_n=1)\log\sigma(a_n) + \mathbb{I}(\tilde y_n=-1)\log\sigma(-a_n) \Bigr] \tag{10.11} \\
&= -\frac{1}{N}\sum_n \log\sigma(\tilde y_na_n) = \frac{1}{N}\sum_n\log(1+e^{-\tilde y_na_n}) \tag{10.13}
\end{align*}
$$
> This is called the **log loss** in `scikit-learn`.



#### 10.2.3.2 Optimizing the objective

We can use any <u>gradient-based optimization</u> algorithm to find the MLE, i.e., to solve
$$
\nabla_w\operatorname{NLL}(w)\equiv g(w)=0 \tag{10.14}
$$



#### 10.2.3.3. Deriving the gradient

Note that $\dfrac{d\sigma}{da}(a)=\sigma(a)(1-\sigma(a))$. Hence by the chain rule we have
$$
\nabla_w\log\sigma(a_n)=(1-\sigma(a_n))x_n \quad\text{and}\quad
\nabla_w\log(1-\sigma(a_n))=-\sigma(a_n)x_n
$$
Thus the gradient vector of the NLL (as a column vector) is given by
$$
\begin{align*}
\nabla_w\operatorname{NLL}(w) &= -\frac{1}{N} \sum_n\bigl[y_n(1-\sigma(a_n))x_n-(1-y_n)\sigma(a_n)x_n\bigr] \tag{10.19} \\
&= \frac{1}{N} \sum_n (\sigma(a_n)-y_n)x_n \tag{10.21} \\
&= \frac{1}{N} X^T\operatorname{diag}[\sigma(a_n)-y_n]1_N \tag{10.22}
\end{align*}
$$
where $X$ is the design matrix of shape $(N,D)$, i.e., the rows of $X$ are $x_n^T$.

If we interpret $e_n\equiv\sigma(a_n)-y_n$ as an <u>error signal</u>, we can see that the gradient weights each input $x_n$ by its error, and then averages the result.



#### 10.2.3.4 Deriving the Hessian

We will prove that the Hessian $H(w)_{ij}\equiv\dfrac{\partial^2\operatorname{NLL}}{\partial w_iw_j}(w)$ is <u>positive definite</u> so that the NLL is <u>strictly convex</u>.
$$
H(w) = \nabla_w\nabla_w^T\operatorname{NLL}(w) = \frac{1}{N} \sum_n \sigma(a_n)(1-\sigma(a_n))x_nx_n^T = \frac{1}{N}X^TSX \tag{10.23}
$$
where $S\equiv\operatorname{diag}[\sigma(a_n)(1-\sigma(a_n))]$. Thus, if $X$ is a <u>full rank</u> matrix, for any nonzero vector $v$, we have
$$
v^T(X^TSX)v = (v^TX^TS^\frac{1}{2})(S^\frac{1}{2}Xv) = \|S^\frac{1}{2}Xv\|_2^2 > 0 \tag{10.25}
$$

However, in practice, $\sigma(a_n)$ which are <u>close to 0 or 1</u> might cause the Hessian to be <u>close to singular</u>. We can avoid this by using $\ell_2$ regularization.



### 10.2.4 Stochastic gradient descent

Our goal is to solve the optimization problem
$$
\hat w\equiv\arg\min_w \mathcal{L}(w) \tag{10.26}
$$
where $\mathcal{L}(w)=\operatorname{NLL}(w)$ is the loss function. If we use a minibatch of size 1, the update equation for SGD becomes

$$
w_{t+1} = w_t - \eta_t\nabla_w\operatorname{NLL}(w_t) = w_t-\eta_t(\sigma(a_n)-y_n)x_n \tag{10.28}
$$

Since the objective $\operatorname{NLL}(w)$ is convex, this procedure will converge to the global optimum, provided we decay the learning rate $\eta_t$ at the appropriate rate.



### 10.2.5 Perceptron algorithm

A **perceptron** [Ros58] is a deterministic binary classifier with the decision function
$$
f(x;\theta)=\mathbb{I}(a>0)=\mathbb{I}(w^Tx+b>0) \tag{10.29}
$$
using the Heaviside step function instead of the sigmoid $\sigma(a)$ (compare with eq. 10.5).

Since the Heaviside step function is <u>not differentiable</u>, we <u>cannot</u> use SGD. However, Rosenblatt proposed the **perceptron learning algorithm** instead
$$
w_{t+1} = w_t - \eta_t\big[\mathbb{I}(a>0)-y_n\bigr] x_n \tag{10.30}
$$

> If the bias term $b$ is absorbed into the weight vector $w$, we can set the learning rate $\eta_t=1$, since the magnitude of $w$ <u>does not</u> affect the decision function $f(x;\theta)=\mathbb{I}(w^Tx>0)$.

If the prediction is correct, no change is made, otherwise we move the weights in a direction so as to make the correct answer more likely.

<img src="PRML_Figure_4.7.png" style="zoom: 50%;" />

> [PRML, p.195] Figure 4.7: Illustration of the convergence of the perceptron learning algorithm.
>
> 1. The **top left plot** shows the initial parameter vector $w$ shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classified as belonging to the red class. The data point circled in green is misclassified and so its feature vector is added to the current weight vector, giving the new decision boundary shown in the **top right plot**.
>
> 2. The **bottom left plot** shows the next misclassified point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown in the **bottom right plot** for which all data points are correctly classified.



#### Perceptron convergence theorem

If the training data set is <u>linearly separable</u>, then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps.



#### Comparison with SGD

- **Advantage**: We don't need to compute probabilities, which is <u>suitable for large scale learning</u>.
- **Disadvantage**: It will <u>only converge when the data is linearly separable</u>, whereas SGD for minimizing the NLL for logistic regression will always converge to the globally optimal MLE.



### 10.2.6 Iteratively reweighted least squares

#### Newton's method or the Newton-Raphson method

> The Newton-Raphson method is a <u>root-finding</u> algorithm:
> $$
> x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
> $$
> It can be used to <u>find a minimum or maximum</u> of $f(x)$. Since $f'(x)=0$ at a minimum or maximum, local minima and maxima can be found by applying Newton-Raphson method to the derivative $f'(x)$:
> $$
> x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}
> $$

We focus on the <u>full batch</u> setting (so we assume $N$ is small), since it is harder to make second order methods work in the stochastic setting. The Newton-Raphson method consists of updates of the form
$$
w_{t+1} = w_t - H_t^{-1}g_t \tag{10.31}
$$
where $g_t\equiv\nabla_w\mathcal{L}(w_t)$ and the Hessian $H_t\equiv\nabla_w^2\mathcal{L}(w_t)$ is assumed to be postive-definite.



#### As a weighted least squares problem

We apply the Newton-Raphson method to logistic regression, $\mathcal{L}(w)=\operatorname{NLL}(w)$. From (eq. 10.22 & 10.23),
$$
\begin{align*}
w_{t+1} &= w_t - H_t^{-1}g_t = w_t - \Bigl(\frac{1}{N}X^TS_tX\Bigr)^{-1} \Bigl(\frac{1}{N} X^T(\sigma(a_t)-y)\Bigr) \\
&= w_t + (X^TS_tX)^{-1}X^T(y-\sigma(a_t)) = (X^TS_tX)^{-1}X^T\Bigl[ S_tXw_t+y-\sigma(a_t)\Bigr] \\
&= (X^TS_tX)^{-1}X^TS_tz_t \tag{10.40}
\end{align*}
$$
where $z_t\equiv Xw_t+S_t^{-1}(y-\sigma(a_t))$ called the **working response** and $S_t=\operatorname{diag}\bigl[\sigma(a_t)(1-\sigma(a_t))\bigr]$.
$$
z_{t,n} = w_t^Tx_n + \frac{y_n-\sigma(a_{t,n})}{\sigma(a_{t,n})(1-\sigma(a_{t,n}))} \tag{10.42}
$$
Equation (10.40) is the normal equation for a <u>weighted least squares</u> problem
$$
z = Xw+\epsilon, \quad \mathbb{E}[\epsilon|X]=0, \quad \operatorname{Cov[\epsilon|X]}=S^{-1}
$$
 which is a <u>minimizer of the squared Mahalanobis length</u> of the residual $z-Xw$:
$$
\arg\min_w \|S^\frac{1}{2}(z-Xw)\|^2 = \arg\min_w(z-Xw)^TS(z-Xw) = \arg\min_w\sum_n S_n(z_n-w^Tx_n)^2 \tag{10.43}
$$



#### IRLS algorithm

Because the weighting matrix $S$ is not constant but depends on $w$, we must apply the normal equation (10.40) iteratively, each time using the new weight vector $w_t$ to compute a revised weighing matrix $S_t$. For this reason, the algorithm is known as the **iterative reweighted least squares (IRLS)**.

<img src="algorithm_10.2.png" style="zoom: 50%;" />



#### FIM and Fisher scoring

> The partial derivative of the log likelihood, $\dfrac{\partial}{\partial\theta}\log p(x|\theta)$ is called the the **score**, and the expected value of the score is zero.
> $$
> \mathbb{E}_{x\sim p(x|\theta)}\Bigl[\frac{\partial}{\partial\theta}\log p(x|\theta)\Bigr] = \int \frac{\frac{\partial}{\partial\theta}p(x|\theta)}{p(x|\theta)}p(x|\theta)\,dx = \frac{\partial}{\partial\theta}\int p(x|\theta)\,dx = 0
> $$

The **Fisher information matrix (FIM)** is defined by the <u>covariance of the score</u> function:

$$
F \equiv \mathbb{E}_{x\sim p(x|\theta)} \bigl[\nabla_\theta\log p(x|\theta)\nabla_\theta\log p(x|\theta)^T \bigr] \tag{4.220}
$$
and has the entry
$$
F_{ij} = \mathbb{E}_{x\sim p(x|\theta)} \Bigl[ \Bigl(\frac{\partial}{\partial\theta_i}\log p(x|\theta)\Bigr)\Bigl(\frac{\partial}{\partial\theta_j}\log p(x|\theta)\Bigr) \Bigr] \tag{4.221}
$$
**Theorem 4.7.2.** If $\log p(x|\theta)$ is twice differentiable, and under certain regularity conditions, the FIM is equal to the <u>expected Hessian</u> of the NLL, i.e.,
$$
F_{ij} = -\mathbb{E}_{x\sim p(x|\theta)} \Bigl[ \frac{\partial^2}{\partial\theta_i\partial\theta_j}\log p(x|\theta) \Bigr] \tag{4.222}
$$
Note that **Fisher scoring** is the same as IRLS except we replace the Hessian with the FIM:
$$
w_{t+1} = w_t + \eta_tF^{-1}\nabla_w\operatorname{NLL}(w_t)
$$
Since the FIM is independent of the data, this can be faster for problems with many parameters.



### 10.2.7 MAP estimation

![](figure_10.4.png)

> The **bottom right plot** uses the polynomial degrees (**1**, **2**, **4**, 6, 8, 10).
>
> - **Degree 1**: $\phi(x_0,x_1) = (x_0,x_1)\in\mathbb{R}^2$ and $\hat w = (0.513, 0.119)$
> - **Degree 2**: $\phi(x_0,x_1) = (x_0,x_1, x_0^2,x_0x_1,x_1^2)\in\mathbb{R}^5$ and $\hat w =(2.293,  0.065, 11.916, 15.504,  2.530)$ 
> - **Degree 4**: $\phi(x_0,x_1) = (x_0,x_1, \dotsc,x_0x_1^3,x_1^4)\in\mathbb{R}^{14}$ and $\hat w=(-661330.445, 165024.837, \dotsc, 5691135.286,5091313.309)$

One way to reduce such overfitting is to <u>prevent the weights from becoming so large</u>. We can do this by using a zero-mean Gaussian prior, $p(w)=\mathcal{N}(w|0,\lambda^{-1}I)$, and then using MAP estimation (see $\S$4.5.3).

$$
\operatorname{PNLL}(w) = \operatorname{NLL}(w) + \lambda\|w\|_2^2 \tag{10.48}
$$
This is called $\ell_2$ **regularization** or **weight decay**. The larger the value of $\lambda$, the more the parameters are penalized for being *large* (deviating from the zero-mean prior), and thus the less flexible the model.

![](figure_10.6.png)

> The **bottom right plot** uses the inverse regularization, $C=\lambda^{-1}$ from `numpy.logspace(0, 5, 7, dtype='int64')`, that is, (**1**, 6, 46, **316**,  2154, 14677, **100000**).
>
> - **C = 1**: $\hat w = (0.395, -0.265, \dotsc, 0.579, 0.244)$
> - **C = 316**: $\hat w =(0.736 -0.533, \dotsc, 6.347, 1.778)$ 
> - **C = 100,000**: $\hat w=(-12.642, -2.732, \dotsc, 78.358, 18.320)$

We can compute the MAP estimate by slightly modifying the input to the above gradient based optimization algorithms. The gradient and Hessian of the penalized negative log likelihood have the following forms:
$$
\begin{align*}
\nabla_w\operatorname{PNLL}(w) &= g(w)+2\lambda w \tag{10.49} \\
\nabla_w^2\operatorname{PNLL}(w) &= H(w)+2\lambda I \tag{10.50}
\end{align*}
$$



### 10.2.8 Standardization

The isotropic prior $\mathcal{N}(w|0,\lambda^{-1}I)$ used to prevent overfitting implicitly encodes the assumptions:

- We expect all weights to be similar in magnitude.
- We expect all input features to be similar in magnitude.

However, in many datasets, input features are on different scales. In such cases, it is common to **standardize** the data, to ensure <u>each feature has mean 0 and variance 1</u>.
$$
\operatorname{standarize}(x_{nd}) = \frac{x_{nd}-\hat\mu_d}{\hat\sigma_d} \tag{10.51}
$$

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X) # compute the mean and (biased) std to be used for later scaling
X_tran = scaler.transform(X) # perform standardization by centering and scaling
X_orig = scaler.inverse_transform(X_tran) # scale back the data to the original
```

An alternative is to use **min-max scaling**, in which we rescale the inputs so they lie in the interval $[0,1]$.
$$
\operatorname{min-max\_scaling}(x_{nd}) = \frac{x_{nd}-\min_{n'}\{x_{n'd}\}}{\max_{n'}\{x_{n'd}\}-\min_{n'}\{x_{n'd}\}}
$$

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler() # default feature_range=(0,1)
scaler.fit(X) # compute the min and max to be used for later scaling
X_tran = scaler.transform(X) # scale features of X according to feature_range
X_orig = scaler.inverse_transform(X_tran) # undo the scaling of X
```





## 10.3 Multinomial logistic regression

$$
p(y|x;\theta) = \operatorname{Cat}(y|S(Wx+b)) \tag{10.54}
$$

where $S$ is the softmax function, $W$ is a $C\times D$ weight matrix with $w_c^T$ as rows, and $b$ is a $C$-dim. bias vector. We will ignore the bias term $b$ by prepending each $x$ with 1, and adding $b$ to the first column of $W$. Thus $\theta=W$.

If we let $a=Wx$ be the $C$-dim. vector of **logits**, we can rewrite
$$
p(y=c|x;\theta) = S(a)_c = \frac{e^{a_c}}{\sum_{c'} e^{a_{c'}}} \tag{10.55}
$$
> Since softmax is <u>invariant under translation</u> by the same value in each coordinate $j$,
> $$
> S(a-a_j)_c = \frac{e^{a_c-a_j}}{\sum_{c'}e^{a_{c'}-a_j}} = \frac{e^{a_c}/e^{a_j}}{\sum_{c'}(e^{a_{c'}}/e^{a_j})} = \frac{e^{a_c}}{\sum_{c'}e^{a_{c'}}} = S(a)_c
> $$
> we can set the last weight vector $w_C=0$. In this case, $\theta$ correspond to a weight matrix of size $(C-1)\times D$.



#### Multi-class, multi-label, and multi-output classification

- In multinomial logistic regression ($C>2$), we assume that there is <u>only one true label</u>, i.e., $y\in\{1,\dotsc,C\}$ for an input. This is called **multi-class classification** in `scikit-learn`.

  ```python
  from sklearn.linear_model import LogisticRegression
  LogisticRegression(multi_class='multinomial').fit(X, y)
  ```

- For some applications (e.g., image tagging), we want to <u>predict one or more labels</u>; in this case, $y\subseteq\{1,\dotsc,C\}$ for an input. This is called **multi-label classification** in `scikit-learn`. 
  Similar to one-hot encoding, $y$ can be viewed as a <u>binary vector</u> in $\{0,1\}^C$. However, it is not required that $\sum_c y_c=1$. We can use a modified version of binary logistic regression with multiple outputs:
  $$
  p(y|x;\theta) = \prod_c \operatorname{Ber}(y_c|\sigma(w_c^Tx)) \tag{10.56}
  $$
  ```python
  from sklearn.preprocessing import MultiLabelBinarizer
  y = MultiLabelBinarizer().fit_transform(y) # make binary vectors
  from sklearn.multiclass import OneVsRestClassifier
  OneVsRestClassifier(LogisticRegression()).fit(X, y)
  ```
  
- If we want to predict <u>fixed number of targets</u> $y=(y_1,\dotsc,y_T)$ with $y_t\in\{1,\dotsc,C\}$ for an input, it is called **multi-class multi-output classification** in `scikit-learn`. On the contrary, the number of targets (or labels) varies by samples in multi-label classification.

  ```python
  from sklearn.multioutput import MultiOutputClassifier
  MultiOutputClassifier(LogisticRegression(multi_class='multinomial')).fit(X, y)
  ```



### 10.3.1 Linear and nonlinear classifiers

![](figure_10.7.png)

> Example of $C=3$ class multinomial logistic regression with $D=2$ dim. inputs. The plots above are generated using the <u>cross-entropy loss</u> (eq. 10.58) <u>without regularization</u>.
>
> ```python
> LogisticRegression(multi_class='multinomial', penalty='none', solver='lbfgs')
> ```
>
> - Multinomial logistic regression computes <u>linear decision boundaries</u>. In the **left plot** (a), no point in the plane is predicted as black label.
> - In the **right plot** (b), $x$ is transformed by $\phi(x_0,x_1)=(x_0,x_1,x_0^2,x_1^2,x_0x_1)\in\mathbb{R}^5$ to create <u>nonlinear boundaries</u>. The plane is divided by three regions each of which is predicted as red, blue, and black label.



### 10.3.2 Maximum likelihood estimation

We discuss how to compute the MLE by minimizing the NLL.



#### 10.3.2.1 Objective

$$
\operatorname{NLL}(W) = -\frac{1}{N}\log\prod_n\prod_c \mu_{nc}^{y_{nc}} = -\frac{1}{N}\sum_n\sum_c y_{nc}\log\mu_{nc} = \frac{1}{N}\sum_n\mathbb{H}(y_n,\mu_n) \tag{10.58}
$$

where $\mu_{nc}\equiv p(y_{nc}=1|x_n;\theta)=S(Wx_n)_c$, and $y_n$ is represented by the one-hot encoding, i.e., $y_{nc}=\mathbb{I}(y_n=c)$ and $\sum_c y_{nc}=1$, and $\mathbb{H}(p,q)\equiv -\sum_c p_c\log q_c$ is the **cross-entropy**. 



#### 10.3.2.2 Optimizing the objective

To find the optimum, we need to solve $\nabla_W\operatorname{NLL}(W)=0$.



#### 10.3.2.3 Deriving the gradient

Since $\mu_c=S(Wx)_c=S(a)_c=\dfrac{e^{a_c}}{\sum_je^{a_j}}$, we have $\dfrac{\partial\mu_c}{\partial a_j}=\mu_c(\delta_{cj}-\mu_j)$, where $\delta_{cj}=\mathbb{I}(c=j)$.

Let $\operatorname{NLL_n(W)}=-\sum_c y_{nc}\log\mu_{nc}$ denote the NLL for a single sample $(x_n, y_n)$. Since the entry $w_{jk}$ in $W$ is associated with class $j$, the gradient of $\operatorname{NLL}_n(W)$ is given by the $C\times D$ matrix
$$
\begin{align*}
\bigl[\nabla_W\operatorname{NLL}_n(W)\bigr]_{jk} &= \frac{\partial}{\partial w_{jk}}\operatorname{NLL}_n(W) = -\sum_c\frac{y_{nc}}{\mu_{nc}}\frac{\partial\mu_{nc}}{\partial w_{jk}} = -\sum_c\frac{y_{nc}}{\mu_{nc}}\frac{\partial\mu_{nc}}{\partial a_{nj}}\frac{\partial a_{nj}}{\partial w_{jk}} \\
&= -\sum_c\frac{y_{nc}}{\mu_{nc}}\mu_{nc}(\delta_{cj}-\mu_{nj})x_{nk} = \sum_c y_{nc}(\mu_{nj}-\delta_{cj})x_{nk} \\
&= \bigl(\sum_c y_{nc}\bigr)\mu_{nj}x_{nk} - y_{nj}x_{nk} = (\mu_{nj}-y_{nj})x_{nk} \tag{10.67}
\end{align*}
$$
The gradient of the overall NLL is obtained by summing over samples, to give the $C\times D$ matrix
$$
g(W) = \frac{1}{N}\sum_n\nabla_{W}\operatorname{NLL}_n(W) = \frac{1}{N}\sum_n (\mu_n-y_n)x_n^T = \frac{1}{N}\sum_n (\mu_n-y_n)\otimes x_n \tag{10.68}
$$



#### 10.3.2.4 Deriving the Hessian

The Hessian of the NLL for multinomial logistic regression is given by the $CD\times CD$ matrix
$$
H(W) = \frac{1}{N} \sum_n \bigl(\operatorname{diag}(\mu_n)-\mu_n\mu_n^T\bigr)\otimes\bigl(x_nx_n^T\bigr) \tag{10.69}
$$
where $A\otimes B$ is the Kronecker product (see $\S$7.2.5), and it is positive definite (Exercise 10.1), so the objective is convex.



### 10.3.3 Gradient-based optimization

- It is straightforward to use the gradient to derive the (first-order) SGD algorithms.
- We can use the Hessian to derive a second-order optimization method.
- However, computing Hessian can be expensive, so it is common to approximate it using **quasi-Newton** methods (see $\S$8.3.2).

The gradient of the log likelihood (eq. 10.68) requires computing normalized probabilities $\mu$, which can be computed from the logits vector $a=Wx$ using
$$
\mu_c=p(y=c|x;\theta)=S(a)_c=\frac{\exp a_c}{\sum_{c'}\exp a_{c'}}=\exp\bigl[a_c-\operatorname{lse}(a)\bigr] \tag{10.73}
$$
where the **log-sum-exp** function is defined by
$$
\operatorname{lse}(a)\equiv\log\sum_c\exp(a_c)=m+\log\sum_c\exp(a_c-m) \tag{2.101}
$$
It is common to use $m=\max_c a_c$ (see $\S$2.5.4).



### 10.3.4 Bound optimization

Let us fit logistic regression using **bound optimization** or **MM (minorize-maximize) algorithm** (see $\S$8.7).

> Our goal is to <u>maximize</u> the log likelihood $\operatorname{LL}(\theta)$. The basic approach is as follows.
>
> 1. Construct a **surrogate function** $Q(\theta,\theta^t)$ which is a tight lower bound to $\operatorname{LL}(\theta)$ such that
> $$
> Q(\theta,\theta^t)\leq\operatorname{LL}(\theta) \quad\text{and}\quad Q(\theta^t,\theta^t)=\operatorname{LL}(\theta^t)
> $$
> 2. If these conditions are met, we say that $Q$ **minorizes** $\operatorname{LL}$.
> 3. Then perform the update at each step:
> $$
> \theta^{t+1} = \arg\max_\theta Q(\theta,\theta^t) \tag{8.144}
> $$
> 4. This guarantees us monotonic increases in the original objective:
> $$
> \operatorname{LL}(\theta^{t+1})\geq Q(\theta^{t+1},\theta^t)\geq Q(\theta^t,\theta^t)=\operatorname{LL}(\theta^t) \tag{8.145}
> $$
>
> ![](figure_8.23.png)
>
> > Figure 8.23: Illustration of a bound optimization algorithm. [Bis06, Figure 9.14]
>
> If $Q$ is a quadratic lower bound, the overall method is similar to Newton's method. The difference is
>
> - Optimizing $Q$ is guaranteed to lead to an improvement in the objective, <u>even if it is not convex</u>.
> - Newton's method may overshoot or lead to a decrease in the objective, <u>since it is a quadratic approximation</u> and not a (lower) bound.



If $\operatorname{LL}(\theta)$ is a <u>concave function we want to maximize</u>, then one way to obtain a valid lower bound is to <u>use a bound on its Hessian</u>, i.e., to find a negative definite matrix $B$ such that $H(\theta)\succ B$. In this case,
$$
\operatorname{LL}(\theta) \geq \operatorname{LL}(\theta^t) + (\theta-\theta^t)^Tg(\theta^t) + \tfrac{1}{2}(\theta-\theta^t)^TB(\theta-\theta^t) \equiv Q(\theta,\theta^t) \tag{10.74}
$$
where $g(\theta^t)=\nabla\operatorname{LL}(\theta^t)$. Then, the update becomes
$$
\theta^{t+1}=\arg\max_\theta Q(\theta,\theta^t)=\theta^t-B^{-1}g(\theta^t) \tag{10.75}
$$


#### Application to logistic regression

We want to <u>maximize the log likelihood</u>
$$
\operatorname{LL}(W) = \sum_n\sum_c y_{nc}\log\mu_{nc}=\sum_n\sum_c y_{nc}(a_c-\operatorname{lse}(a)) \tag{10.76}
$$
The gradient and the Hessian are given by
$$
\begin{align*}
g(W) &= \sum_n (y_n-\mu_n)\otimes x_n \tag{10.77} \\
H(W) &= -\sum_n \bigl(\operatorname{diag}(\mu_n)-\mu_n\mu_n^T\bigr)\otimes\bigl(x_nx_n^T\bigr) \tag{10.78}
\end{align*}
$$
We can construct a lower bound on the Hessian, as shown in [D. Böhning, *Multinomial logistic regression algorithm*, Ann. Inst. Statist. Math. (1992)]
$$
B \equiv -\frac{1}{2}\bigl[I_C-\tfrac{1}{C}11^T\bigr] \otimes \Bigl(\sum_n x_nx_n^T\Bigr) \prec H(W) \tag{10.79}
$$
> If we enforce that $w_C=0$, we can use the $(C-1)\times(C-1)$ matrix $\bigl[I_{C-1}-\frac{1}{C}11^T\bigr]$ instead.

We can use this lower bound to construct an MM algorithm to find the MLE. The update becomes
$$
W^{t+1} = W^t - B^{-1}g(W^t) \tag{10.81}
$$
<u>This iteration can be faster than IRLS</u>, since we can precompute $B^{-1}$ in time independent of $t$, rather than having to invert the Hessian at each iteration.

> In the binary case when $C=2$, assume that $w_2=0$. Then, since $\mu_n-\mu_n^2=\mu_n(1-\mu_n)\leq\frac{1}{4}$ for $0\leq\mu_n\leq 1$, we have
> $$
> H(w)=-\sum_n(\mu_n-\mu_n^2)x_nx_n^T \succ -\sum\tfrac{1}{4}x_nx_n^T=-\tfrac{1}{4}X^TX\equiv B
> $$
> The update becomes $w^{t+1}=w^t+4(X^TX)^{-1}g(w^t)=w^t+4(X^TX)^{-1}X^T(y-\mu_t)$. Compare this to (eq. 10.37) $w_{t+1}=w_t+(X^TS_tX)^{-1}X^T(y-\mu_t)$, where $S_t=\operatorname{diag}(\mu_t\odot(1-\mu_t))$.



### 10.3.5 MAP estimation

The benefits of $\ell_2$ regularization for binary logistic regression hold also in the multinomial case. However, there is an **identifiability** issue.

> A model is **identifiable** if it is theoretically possible to learn the true values of this model's underlying parameters after obtaining an infinite number of observations from it.
>
> Mathematically, this is equivalent to saying that <u>different values of the parameters</u> must generate <u>different probability distributions</u> of the observable variables.

Recall that multinomial logistic regression has the form
$$
p(y=c|x;W) = S(Wx) = \frac{\exp(w_c^Tx)}{\sum_{c'}\exp(w_{c'}^Tx)} \tag{10.84}
$$
where $W$ is a $C\times D$ weight matrix with rows $w_c^T$. We can arbitrarily *clamp* $w_c=0$ for one of the classes, since softmax is invariant under translation by the same value in each coordinate. Therefore, <u>the parameters will be unidentifiable if we don't *clamp*</u>.



However, suppose <u>we don't *clamp*</u> $w_c=0$, but <u>we add $\ell_2$ regularization</u> by optimizing
$$
\operatorname{PNLL}(W) = \operatorname{NLL}(W) + \lambda\|W\|_F^2 \tag{10.86}
$$
where $\|W\|_F=\sqrt{\sum_{i,j}w_{ij}^2}$ is the **Frobenius norm** (eq. 7.27). At the optimum $\hat W$, it is required that
$$
\nabla_W\operatorname{PNLL}(\hat W) = \nabla_W\operatorname{NLL}(\hat W) + 2\lambda \hat W = \frac{1}{N}\sum_n(\hat\mu_n-y_n)\otimes x_n + 2\lambda \hat W = 0 \tag{10.87}
$$
so that $\sum_n (y_n-\hat\mu_n)\otimes x_n=2N\lambda\hat W$, where $\hat\mu_n=S(\hat Wx_n)$. It follows that, for any feature dimension $j$,
$$
2N\lambda\sum_c\hat W_{cj} = \sum_n\sum_c(y_{nc}-\hat\mu_{nc})x_{nj} = \sum_n\Bigl[\sum_c y_{nc}-\sum_c\hat\mu_{nc}\Bigr]x_{nj} = \sum_n(1-1)x_{nj} = 0 \tag{10.89}
$$
Thus if $\lambda>0$ we should have $\sum_c\hat W_{cj}=0$, so the weights <u>automatically satisfy a sum-to-zero constraint</u>, thus making them <u>uniquely identifiable</u>. (The softmax function and thus $\mu_n$ and $\operatorname{NLL}(W)$ are invariant under translation by the same value in each coordinate; whereas $\operatorname{PNLL}(W)$ is not invariant.)



### 10.3.6 Maximum entropy classifiers

There is a slight <u>extension of logistic regression</u> that use class-dependent features,
$$
p(y|x;w) = \frac{\exp\bigl(w^T\phi(x,y)\bigr)}{\sum\exp\bigl(w^T\phi(x,y)\bigr)} \tag{10.91}
$$
where $\phi(x,y)$ is the feature vector of $x$ for class $y$. This is called a **maximum entropy classifier**, or **maxent classifier** for short. Notice that $\theta=w$ is a vector, not a matrix $W$.

Maxent classifiers are very widely used in NLP. For example, consider the problem of **semantic role labeling**, where we classify a word $x$ into a semantic role $y$ (person, place or thing).
$$
\begin{align*}
\phi_1(x,y) &= \mathbb{I}(y=\text{person $\and$ $x$ occcurs after Mr. or Mrs.}) \\
\phi_2(x,y) &= \mathbb{I}(y=\text{person $\and$ $x$ is in whitelist of common names}) \\
\phi_3(x,y) &= \mathbb{I}(y=\text{place $\and$ $x$ is in Google maps}) \\
&\vdots
\end{align*}
$$



### 10.3.7 Hierarchical classification

> **Reference**: [RF17] J. Redmon and A. Farhadi, *YOLO9000: Better, Faster, **Stronger***, CVPR 2017.
>
> - (Small) **detection dataset**: **COCO** is a large-scale object detection, segmentation, and captioning dataset with 330K images (over 200K labeled), 80 object categories, etc.
> - (Big) **classification dataset**: **ImageNet** is an image dataset organized according to the WordNet hierarchy. There are more than 100,000 synsets in WordNet. In ImageNet, we aim to provide on average 1000 images to illustrate each synset.
>
> We would like <u>object detection to scale to level of object classification</u>.
>
> - A <u>hierarchical view</u> of object classification that allows us to combine distinct datasets together.
> - <u>Detection images</u> to learn to precisely localize objects; <u>classification images</u> to increase its vocabulary and robustness.

<img src="YOLO9000_figure_6.png" style="zoom:50%;" />

We might want to predict what kind of an animal is in an image: it could be a `Dog` or a `Cat`; if it is a `Dog`, it could be a `Golden Retriever` or `German Shepherd`, etc.

<img src="https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_10.8.png" style="zoom:50%;" />



1. Create model with a binary output label for every possible node in the tree (similar to multinomial logistic regression).

2. **Label smearing**: A label is propagated to all of its parents (hypernyms). For example, if an image is labeled `Golden Retriever`, we will also label it `Dog` and `Mammals` (similar to multi-label classification).

   However, the model <u>does not</u> capture the fact that some labels are mutually exclusive. For example, this method could predict `Golden Retriever`, `Cat`, and `Birds` all with probability 1.0.

3. Use the softmax function to add a mutual exclusion constraint between all label nodes which are siblings. For example,
   $$
   p(y=\text{Dog}|x;\text{Mammals}) = S(a)_\text{Dog} = \frac{\exp(a_\text{Dog})}{\exp(a_\text{Dog})+\exp(a_\text{Cat})+\dotsc}
   $$
   Furthermore,
   $$
   p(\text{Golden Retriever}|x) = p(\text{Golden Retriever}|x,\text{Dog}) \times p(\text{Dog}|x,\text{Mammals}) \times p(\text{Mammals}|x)
   $$
   <img src="YOLO9000_figure_5.png" style="zoom:50%;" />



### 10.3.8 Handling large numbers of classes

#### 10.3.8.1 Hierarchical softmax

In regular softmax classifiers, computing the normalization constant, which is need to compute $\nabla_W\operatorname{NLL}(W)$, takes $O(C)$ time, which can <u>become the bottleneck if $C$ is large</u>.

![](https://qph.cf2.quoracdn.net/main-qimg-d07ca409f4d91c87a1cca5c9ca89ea6a-pjlq)

However, if we structure the labels as a tree, we can compute the probability of any label in $O(\log C)$ time, by multiplying the probabilities of each edge on the path from the root to the leaf.

![](https://qph.cf2.quoracdn.net/main-qimg-4ca2bfbef8d1a278c65efc1be5a3965d)

> Figure 10.9: A flat and **hierarchical softmax** model $p(y=w|x=C)$, where $C$ are the input features (context) and $w$ is the output label (word). https://www.quora.com/What-is-hierarchical-softmax/

A good way to structure such a tree is to use Huffman encoding.

Notice that in the flat softmax, the parameter $W$ consists of the row vectors $w_w^T$ for each leaf (label) $w$. On the contrary, in the hierarchical softmax, $W$ consists of the row vectors $w_j^T$ for each inner node $j$ of the tree.

Let $n_j(w)$ be the $j$-th node on the path from the root to $w$, and let $L(w)$ be the length of this path. Then
$$
\begin{align*}
p(w|x;W) &= \prod_{j=1}^{L(w)-1} p(n_j(w)\to n_{j+1}(w)|x;w_{n_j(w)}) \\
&= \prod_{j=1}^{L(w)-1} \sigma(\mathbb{sgn}[n_j(w)\to n_{j+1}(w)]\cdot w_{n_j(w)}^Tx)
\end{align*}
$$
where $\mathbb{sgn}(n_p\to n_c)$ is 1 if $n_c$ is the left child of $n_p$, otherwise -1.



#### 10.3.8.2 Class imbalance and the long tail

**Class imbalance**: Another issue that often arises is that <u>we may have very few samples for most classes</u>. More precisely, the empirical distribution $p(N_1,\dotsc, N_C)$ may have a **long tail**.

Since the rare classes will have a smaller effect on the overall loss than the common classes, <u>the model may focus its attention on the common classes</u>.

1. Set the bias terms $b$ such that $\mathcal{S}(b)_c = \dfrac{N_c}{N}$; such a model will match the empirical label prior even when using weights of $w=0$. As the weights are adjusted, the model can learn input-dependent deviations from this prior.
2. Resample the data to make it more balanced, before (or during) training. In particular suppose we sample a data point from class $c$ with probability $p_c=\dfrac{N_c^q}{\sum_j N_j^q}$.
   - If we set $q=1$, we recover standard **instance-balanced sampling**, where $p\propto N_C$; the common classes will be sampled more than rare classes.
   - If we set $q=0$, we recover **class-balanced sampling**, where $p_c=\frac{1}{C}$; this can be thought of as first sampling a class uniformly at random, and then sampling an instance of this class.
   - We can consider other options, such as $q=0.5$, which is known as **square-root sampling**.
3. Another method that is simple and can easily handle the long tail is to use the **nearest class mean classifier**.





## Implementation of logistic regression in scikit-learn

> Classification can be viewed as a two step process:
>
> 1. Model construction: $x\in\mathbb{R}^D\mapsto a=\phi(x;\theta)\in\mathbb{R}^*$, $\phi$ can be any function including DNN
>    - **Binary classification**: $x\mapsto a=w^Tx+b\in\mathbb{R}$, where $\theta=(w,b)\in\mathbb{R}^{D+1}$ and $y\in\{0,1\}$ 
>    - **Multi-class classification**: $x\mapsto a=Wx+b\in\mathbb{R}^C$, where $\theta=(W,b)\in\mathbb{R}^{C(D+1)}$ and either $y\in\{1,2,\dotsc,C\}$ or $y\in\{0,1\}^C$ with $\sum_c y_c=1$ (one-hot encoding)
>    - **Multi-label classification**: $x\mapsto a=Wx+b\in\mathbb{R}^C$, where $\theta=(W,b)\in\mathbb{R}^{C(D+1)}$ and $y\in\{0,1\}^C$
>    - **Multi-class multi-output classification**: $x\mapsto [a_t=W_tx+b_t]\in\mathbb{R}^{TC}$, where $\theta=(W_t,b_t)\in\mathbb{R}^{TC(D+1)}$ and $[y_t]\in\mathbb{R}^T$ with $y_t\in\{1,2,\dotsc,C\}$
> 2. Parameter learning: $a\mapsto \mu\mapsto p(y|x;\theta)$. In particular $\operatorname{NLL}(\theta)\equiv-\frac{1}{N}\sum_n\log p(y_n|x_n;\theta)$ gives a loss to learn the parameters $\theta$.
>    - **Binary classification**: $a\mapsto\mu=\sigma(a)\in\mathbb{R}\mapsto\operatorname{Ber}(y|\mu)$, and $\operatorname{NLL}(\theta)=\frac{1}{N}\sum_n\mathbb{H}(y_n,\mu_n)$
>    - **Multi-class classification**: $[a_c]\mapsto\mu=S([a_c])\in\mathbb{R}^C\mapsto\operatorname{Cat}(y|\mu)$, and $\operatorname{NLL}(\theta)=\frac{1}{N}\sum_n\mathbb{H}(y_n,\mu_n)$
>    - **Multi-label classification**: $[a_c]\mapsto[\mu_c=\sigma(a_c)]\in\mathbb{R}^C\mapsto\prod_c\operatorname{Ber}(y|\mu_c)$, and $\operatorname{NLL}(\theta)=\frac{1}{N}\sum_n\sum_c\mathbb{H}(y_{nc},\mu_{nc})$
>    - **Multi-class multi-output classification**: $[a_{tc}]\mapsto [\mu_t=S([a_{tc}]_c)]_t\in\mathbb{R}^{TC}\mapsto\prod_t\operatorname{Cat}(y_t|\mu_t)$, and $\operatorname{NLL}(\theta)=\frac{1}{N}\sum_n\sum_t\mathbb{H}(y_{nt},\mu_{nt})$



Our goal is to solve the optimization problem
$$
\hat w\equiv\arg\min_w \bigl[\mathcal{L}(w) + \alpha \mathcal{R}(w)\bigr]
$$
where $\mathcal{L}(w)$ is the loss function that measures model (mis)fit and $\mathcal{R}$ is a regularization term (or `penalty`) that penalizes model complexity, where $\alpha>0$ is a hyperparameter that controls the regularization strength.


```python
from sklearn.linear_model import SGDClassifier
clf = SGDClassifier(loss='log_loss', penalty='none')
    # e.g., learning_rate='constant', eta0=0.1 for constant learning rate
clf.fit(X, y)
clf.decision_function(X) # logit a: X @ clf.coef_.T + clf.intercept_
clf.predict(X) # argmax of decision_function(X)
clf.predict_proba(X) # softmax of decision_function(X)
    # For the binary classification: (1-p, p), where p=sigmoid(decision_function(X))
    # In this case, decision_function(X) is of shape (1,)
clf.predict_log_proba(X) # log of predict_proba(X)
```

- `clf.coef_`: Weights assigned to the features, of shape $(C,D)$ or $(1,D)$ if binary
- `clf.intercept_`: Constants in decision function, of shape $(C,)$ or $(1,)$ if binary



We can improve the convergence speed using variance reduction techniques such as **SAGA** (see $\S$8.4.5).

> SAGA – Defazio, A., Bach F. & Lacoste-Julien S. (2014). https://arxiv.org/abs/1407.0202

In particular, `LogisticRegression` in `scikit-learn` solves the optimization problem
$$
\arg\min_w \Bigl[ C \sum_n \log\bigl[1+\exp(-\tilde y_n(w^Tx_n+b))\bigr] + \mathcal{R}(w) \Bigr]
$$
where $C$  (default 1.0) gives the inverse of regularization strength.

```python
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(solver='saga', penalty='none')
    # Supported penalties by 'saga': 'elasticnet', 'l1', 'l2', 'none'
clf.fit(X, y)
clf.decision_function(X) # logit a: X @ clf.coef_.T + clf.intercept_
clf.predict(X) # argmax of decision_function(X)
clf.predict_proba(X) # softmax of decision_function(X)
    # For the binary classification: (1-p, p), where p=sigmoid(decision_function(X))
    # In this case, decision_function(X) is of shape (1,)
clf.predict_log_proba(X) # log of predict_proba(X)
```

- `clf.coef_`: Coefficient of the features in the decision function, of shape $(C,D)$ or $(1,D)$ if binary
- `clf.intercept_`: Intercept (bias) added to the decision function, of shape $(C,)$ or $(1,)$ if binary



Notice that `sklearn.linear_model.Perceptron()` is equivalent to

````python
SGDClassifier(loss='perceptron', penalty='none', learning_rate='constant', eta0=1.0)
````

The `perceptron` loss is defined by
$$
\mathcal{L}(w) = \frac{1}{N}\sum_n\max\{0,-\tilde y_na_n\}
$$


```python
from sklearn.linear_model import Perceptron
clf = Perceptron(penalty='none') # Supported penalties: 'elasticnet', 'l1', 'l2'
clf.fit(X, y)
clf.decision_function(X) # logit a: X @ clf.coef_.T + clf.intercept_
clf.predict(X) # argmax of decision_function(X)
```

- `clf.coef_`: Coefficient of the features in the decision function, of shape $(C,D)$ or $(1,D)$ if binary
- `clf.intercept_`: Intercept (bias) added to the decision function, of shape $(C,)$ or $(1,)$ if binary

