# 9. Linear Discriminant Analysis

Consider a **classification model** called a **generative classifier**:
$$
p(y=c|x;\theta) = \frac{p(x|y=c;\theta)p(y=c;\theta)}{\sum_{c'}p(x|y=c';\theta)p(y=c';\theta)} \tag{9.1}
$$


## 9.2 Gaussian discriminant analysis (GDA)

Assume that the **class conditional densities** are multivariate Gaussians, i.e.,
$$
p(x|y=c;\theta) = \mathcal{N}(x|\mu_c,\Sigma_c) \tag{9.2}
$$
Then the **class posterior** has the form
$$
p(y=c|x;\theta) \propto \pi_c\,\mathcal{N}(x|\mu_c,\Sigma_c) 
\tag{9.3}
$$
where $\pi_c=p(y=c)$ is the given **class prior**.



### 9.2.4 Model fitting

How to fit a GDA model $p(x,y|\theta)$ using **MLE**:
$$
\begin{align*}
p(\mathcal{D}|\theta) &= \prod_n p(x_n,y_n|\theta) = \prod_n \Bigl[ \operatorname{Cat}(y_n|\pi)\,\prod_c\mathcal{N}(x_n|\mu_c,\Sigma_c)^{\mathbb{I}(y_n=c)} \Bigr] \tag{9.17} \\
\log p(\mathcal{D}|\theta) &= \sum_c N_c\log\pi_c + \sum_c \sum_{n:y_n=c} \log\mathcal{N}(x_n|\mu_c,\Sigma_c) \tag{9.18}
\end{align*}
$$
Optimize $\pi_c$ ($\S$4.2.4) and $(\mu_c,\Sigma_c)$ ($\S$4.2.6) separately:
$$
\begin{align*}
\hat\pi_c &= \frac{N_c}{N}, \qquad
\hat\mu_c = \frac{1}{N_c}\sum_{n:y_n=c} x_n, \text{\quad (sample mean)} \tag{9.19} \\
\hat\Sigma_c &= \frac{1}{N_c}\sum_{n:y_n=c} (x_n-\hat\mu_c)(x_n-\hat\mu_c)^T \text{\quad (empirical covariance)} \tag{9.20}
\end{align*}
$$

Given a sample matrix $X$ of shape $(N,D)=$ `(n_samples, n_features)`, notice that

```python
numpy.cov(X, rowvar=False) # (unbiased) sample covariance
numpy.cov(X, rowvar=False, bias=True) # (biased) empirical covariance
sklearn.covariance.empirical_covariance(X) # the same as above (biased)
```



### 9.2.1 Quadratic decision boundaries

The log posterior over class labels (eq. 9.3) is called the **discriminant function**,
$$
\log p(y=c|x;\theta) \propto \log\pi_c -\frac{1}{2}\log|\Sigma_c|-\frac{1}{2}(x-\mu_c)^T\Sigma_c^{-1}(x-\mu_c) + \text{const} \tag{9.4}
$$
The decision boundary between any two classes will be a <u>quadratic function</u> of $x$. Hence this is known as **quadratic discriminant analysis (QDA)**.

```python
from sklearn import QuadraticDiscriminantAnalysis

clf = QuadraticDiscriminantAnalysis(reg_param=0.0)
clf.fit(X, y) # use (unbiased) sample covariance, not biased one
clf._decision_function(X) # discriminant function w/o const (eq. 9.4)
clf.decision_function(X) # same as _decision_function(X)
    # for the binary classification: log p(y=1|x) - log p(y=0|x)
clf.predict(X) # argmax of _decision_function(X)
clf.predict_proba(X) # softmax of _decision_function(X), p(y=c|x)
clf.predict_log_proba(X) # log of predict_proba(X), log p(y=c|x)
```

Let $X_c$ be the (centered) sample matrix for the class label $c$ , i.e., the rows of $X_c$ are $(x_n-\hat\mu_c)^T$ for $\{n: y_n=c\}$. Note that $X_c$ is of shape $(N_c,D)$.

Let $X_c=USV^T$ be the (reduced) singular value decomposition. Then the <u>sample covariance</u> for the class label $c$ (`clf.covariance_[c]`) $\hat\Sigma_c$ satisfies
$$
\hat\Sigma_c = \frac{1}{N_c-1} \sum_{n:y_n=c} (x_n-\hat\mu_c)(x_n-\hat\mu_c)^T = \frac{X_c^T X_c}{N_c-1} = \frac{(VSU^T)(USV^T)}{N_c-1} = V\frac{S^2}{N_c-1}V^T
$$

- `clf.scalings_`: eigenvalues of $\hat\Sigma_c$, that is $\frac{S^2}{N_c-1}$ of shape $(D,)$
- `clf.rotations_`: eigenvectors of $\hat\Sigma_c$, that is $V$of shape $(D,D)$

Note that `reg_param` $0\leq\lambda\leq1$ controls `clf.scalings_` using $(1-\lambda)\frac{S^2}{N_c-1}+\lambda$, i.e., $\hat\Sigma_c$ for $\lambda=0$ and the identity matrix $I_D$ for $\lambda=1$.



### Problem: The MLE for $\hat\Sigma_c$ can easily <u>overfit</u> if $N_c$ is small compared to $D$.



### 9.2.2 Linear decision boundaries

If $\Sigma_c=\Sigma$ for all classes $c$, the log posterior over class labels (eq. 9.4) is simplified to a <u>linear function</u> of $x$.
$$
\begin{align*}
\log p(y=c|x;\theta) &\propto \log\pi_c - \frac{1}{2}(x-\mu_c)^T\Sigma^{-1}(x-\mu_c) + \text{const} \tag{9.5} \\
&= \Bigl[\log\pi_c - \frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c \Bigr] + \Bigl[ x^T\Sigma^{-1}\mu_c \Bigr] + \Bigl[ \text{const} - \frac{1}{2}x^T\Sigma^{-1}x \Bigr] \tag{9.6} \\
&\equiv \gamma_c + x^T\beta_c + \text{const indep. of $c$} \tag{9.7}
\end{align*}
$$
Hence this method is called **linear discriminant analysis (LDA)**. Note that
$$
\beta_c\equiv\Sigma^{-1}\mu_c \quad\text{and}\quad \gamma_c\equiv\log\pi_c-\frac{1}{2}\mu_c^T\beta_c
$$


#### 9.2.4.1 Tied (or shared) covariance

Use the weighted sum $\sum_c\pi_c\hat\Sigma_c$ of <u>empirical covariances</u> as a shared covariance (`clf.covariance_`) $\hat\Sigma$. In particular, when $\pi_c=\frac{N_c}{N}$ (MLE), we have the within-class scatter matrix $\frac{1}{N}S_W$:
$$
\hat\Sigma \equiv \sum_c \pi_c\hat\Sigma_c = \sum_c \frac{N_C}{N} \hat\Sigma_c = \frac{1}{N} \sum_c \sum_{n:y_n=c} (x_n-\hat\mu_c)(x_n-\hat\mu_c)^T \equiv \frac{1}{N} S_W \tag{9.21}
$$

```python
from sklearn import LinearDiscriminantAnalysis

clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None)
clf.fit(X, y)
clf.decision_function(X) # discriminant function w/o const (eq. 9.7)
clf.predict(X) # argmax of decision_function(X)
clf.predict_proba(X) # softmax of decision_function(X)
    # For the binary classification: (1-p, p), where p=sigmoid(decision_function(X))
    # In this case, decision_function(X) is of shape (1,) (see below)
clf.predict_log_proba(X) # log of predict_proba(X)
```

- `clf.coef_`: of shape $(C,D)$. Each row is the <u>least squares solution</u> of $\hat\mu_c=\hat\Sigma\cdot x$, that is, $\beta_c$ (eq. 9.7).
- `clf.intercept_`: of shape $(C,)$ with values $\gamma_c$ (eq. 9.7).

In binary classification, `clf.coef_` is of shape `(1,D)`, that is, `clf.coef_[1, :] - clf.coef_[0, :]`, and `clf.intercept_` is of shape `(1,)`, that is, `clf.intercept_[1] - c.intercept_[0]`.



#### 9.2.4.2 Diagonal covariance

If we force $\hat\Sigma_c$ to be diagonal, we reduce the number of parameters from $O(CD^2)$ to $O(CD)$, which <u>avoids the overfitting problem</u>. However, this <u>loses the ability to capture correlation</u> between the features. (This is known as the **naive Bayes assumption**, see $\S$9.3.)

We can further restrict the model capacity by using a <u>shared (tied) diagonal covariance matrix</u>, called **diagonal LDA**. (See [HTF09, p.652])

> [HTF09] T. Hastie, R. Tibshirani, and J. Friedman, *The Elements of Statistical Learning*, 2E, 2009.



#### 9.2.4.3 MAP estimation

In high dimensions where $D\gg N$, the MLE estimate for the covariance can easily become singular, and forcing the covariance matrix to be diagonal is a rather strong assumption.

An alternative approach is to perform MAP estimation of a (shared) full covariance Gaussian, rather than using the MLE. The MAP estimate is (see $\S$4.5.2)
$$
\hat\Sigma_\text{map} = \lambda\operatorname{diag}(\hat\Sigma_\text{mle}) + (1-\lambda)\hat\Sigma_\text{mle} = \begin{cases} \hat\Sigma_\text{mle} & \text{for diagonals} \\ (1-\lambda) \hat\Sigma_\text{mle} & \text{for off-diagonals} \end{cases} \tag{9.22}
$$
where $\lambda$ controls the amount of regularization. This technique is known as **regularized discriminant analysis** or **RDA**. (See [HTF09, p.656])

```python
# Note that shrinkage works only with ‘lsqr’ and ‘eigen’ solvers.
LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.1)
LinearDiscriminantAnalysis(solver='eigen', shrinkage=0.9)
```

- `shrinkage=None`: (default) $\hat\Sigma\equiv\hat\Sigma_\text{mle}=\frac{1}{N}S_W$ (eq. 9.21)
- `shrinkage=lambda`: $\hat\Sigma\equiv\hat\Sigma_\text{map}=\lambda\operatorname{diag}(\hat\Sigma_\text{mle}) + (1-\lambda)\hat\Sigma_\text{mle}$ (eq. 9.22)
- `shrinkage='auto'`: Use the **Ledoit-Wolf lemma** (see $\S$4.5.2.1)



### 9.2.5 Nearest centroid classifier

Assume a uniform prior over classes, i.e., $\pi_c=\pi$ for all classes $c$. Then the most probable class label (eq. 9.5) is
$$
\hat y(x) = \operatorname{argmax}_c \log p(y=c|x;\theta) = \operatorname{argmin}_c (x-\mu_c)^T\Sigma^{-1}(x-\mu_c) = \operatorname{argmin}_c d^2(x,\mu_c)
$$
where $d$ is the Mahalanobis distance. This is called the **nearest centroid classifier**, or **nearest class mean classifier (NCM)**.

We can replace $d$ with any other distance metric. One simple approach to learn $d$ is to use (following the notation of [Men+12])
$$
d^2(x,\mu_c) = \|x-\mu_c\|_W^2 = \|W(x-\mu_c)\|^2 = (x-\mu_c)^T(W^TW)(x-\mu_c)
$$
The corresponding class posterior becomes
$$
p(y=c|x;\mu,W) = \frac{\exp(-\frac{1}{2}\|W(x-\mu_c)\|^2)}{\sum_{c'}\exp(-\frac{1}{2}\|W(x-\mu_{c'})\|^2)}
$$
We can optimize $W$ using <u>gradient descent applied to the discriminative loss</u> $\mathcal{L}$ [Men+12] , where
$$
\begin{align*}
\mathcal{L} &= -\frac{1}{N} \sum_n \ln p(y=y_n|x_n) \\
\nabla_W\mathcal{L} &= \frac{1}{N} \sum_n\sum_c \bigl[ \mathbb{I}(y_n=c)-p(y=c|x_n) \bigr] W(x_n-\mu_c)(x_n-\mu_c)^T
\end{align*}
$$
This is called **nearest class mean metric learning**. The advantage of this technique is that it can be used for **one-shot learning** of new classes, since we just need to see a single labeled prototype $\mu_c$ per class (assuming we have learned a good $W$ already).



### Problem: GDA is problematic in <u>high dimensions</u>.



### 9.2.6 Fisher's linear discriminant analysis*

1. <u>Reduce the dimensionality of the features</u> $x\in\mathbb{R}^D$ and then fit an MVN to the resulting low-dimensional features $z\in\mathbb{R}^K$. The simplest approach is to use a linear projection (e.g., PCA) $z=W^Tx$, where $W$ is a $D\times K$ matrix.
2. Use <u>gradient based methods</u> to optimize the log likelihood, derived from the class posterior in the low dimensional space as $\mathcal{L}$ in $\S$9.2.5.
3. (**Fisher's linear discriminant analysis**, or **FLDA**) Find $W$ such that the low-dimensional data can be classified as well as possible using a Gaussian class-conditional density model.

FLDA is an interesting <u>hybrid of discriminative and generative techniques</u>. The drawback is that it is restricted to using $K\leq C-1$ dimensions, regardless of $D$.



#### 9.2.6.2 Extension to higher dimensions and multiple classes [DHS01]

Let $\mu_c=\frac{1}{N_C}\sum_{n:y_n=c} x_n$ be the class mean and $\mu=\frac{1}{N}\sum_c N_c\mu_c=\frac{1}{N}\sum_n x_n$ be the overall mean. Define the **between-class scatter**
$$
S_B\equiv S_T-S_W=\sum_c N_c(\mu_c-\mu)(\mu_c-\mu)^T
$$
where
$$
\begin{align*}
S_T &= \sum_n (x_n-\mu)(x_n-\mu)^T \text{\quad (\textbf{total scatter})} \\
S_W &= \sum_c \sum_{n:y_n=c} (x_n-\mu_c)(x_n-\mu_c)^T \text{\quad (\textbf{within-class scatter})}
\end{align*}
$$

1. $\frac{1}{N}S_T$ is the (total) empirical covariance
2. $\frac{1}{N}S_W=\sum_c\frac{N_c}{N}\hat\Sigma_c$ is the weighted sum of empirical covariances over classes (eq. 9.21)
3. $\frac{1}{N}S_B=\sum_c\frac{N_c}{N} (\mu_c-\mu)(\mu_c-\mu)^T$

The projection from $x\in\mathbb{R}^D$ to $z\in\mathbb{R}^K$ is accomplished by $K\leq C-1$ discriminant functions $z_k=w_k^T x$. If the weight vectors $w_k$ are viewed as the columns of $D\times K$ matrix $W$, then the projection can be written as $z=W^Tx$.

Define $m_c=\frac{1}{N_c}\sum_{n:y_n=c} z_n$ and $m=\frac{1}{N}\sum_c N_c m_c=\frac{1}{N}\sum_n z_n$. Then
$$
\begin{align*}
\tilde S_B &= \sum_c N_c (m_c-m)(m_c-m)^T = W^TS_BW \\
\tilde S_W &= \sum_c\sum_{n:y_n=c} (z_n-m_c)(z_n-m_c)^T = W^TS_WW
\end{align*}
$$
<u>The goal is to find a transformation matrix $W$</u> that maximize the ratio of the between class scatter to the within-class scatter. A simple scalar measure of scatter is the determinant of the scatter matrix
$$
J(W) = \frac{|\tilde S_B|}{|\tilde S_W|} = \frac{|W^TS_BW|}{|W^TS_WW|}
$$
[https://arxiv.org/abs/1903.11240] According to Rayleigh-Ritz quotient method, the optimization problem:
$$
\max_w J(w) \equiv \max_w \frac{w^TS_Bw}{w^TS_Ww} \tag{9.30}
$$
can be restated as:
$$
\max_w w^TS_Bw \quad\text{subject to}\quad w^TS_Ww=1
$$
The Lagrangian is $\mathcal{L} = w^TS_Bw - \lambda(w^TS_Ww-1)$, where $\lambda$ is the Lagrange multiplier. Then
$$
\frac{\partial\mathcal{L}}{\partial w} = 2S_Bw - 2\lambda S_Ww = 0 \quad\Longrightarrow\quad S_B w = \lambda S_W w
$$
Thus <u>the columns of an optimal $W$are the generalized eigenvectors $w_k$</u> that correspond to the largest eigenvalues $\lambda_k$ in
$$
S_B w_k = \lambda_k S_W w_k
$$
Note that $\tilde S_B=W^TS_BW=\operatorname{diag}(\lambda_k)$ and $\tilde S_W=W^TS_WW=I_K$ so that
$$
J(W) = \frac{|\tilde S_B|}{|\tilde S_W|} = \frac{|W^TS_BW|}{|W^TS_WW|} = \prod_k\lambda_k
$$
In general the solution for $W$ is not unique.

```python
clf = LinearDiscriminantAnalysis(solver='eigen', shrinkage=None)
clf.fit(X, y)
X_trans = clf.transform(X) # dimension reduction, X @ clf.scalings_ = X @ W
clf.decision_function(X) # discriminant function w/o const (eq. 9.7)
clf.predict(X) # argmax of decision_function(X)s
clf.predict_proba(X) # softmax of decision_function(X)
    # For the binary classification: (1-p, p), where p=sigmoid(decision_function(X))
    # In this case, decision_function(X) is of shape (1,) (see below)
clf.predict_log_proba(X) # log of predict_proba(X)
```

- `clf.scalings_`: $W$ of shape $(D,K)$. Generalized eigenvectors (as columns) that correspond to the `K` largest eigenvalues in $S_Bw_k=\lambda_k S_W w_k$.
- `clf.coef_`: of shape $(C,D)$. Each row is $\beta_c=WW^T\mu_c$ so that $x^T\beta_c=x^TWW^T\mu_c=z^TI_k^{-1}m_c$ (eq. 9.7).
- `clf.intercept_`: of shape $(C,)$ with values $\gamma_c$ so that $\mu_c^T\beta_c=\mu_c^TWW^T\mu_c=m_c^TI_k^{-1}m_c$ (eq. 9.7). 

In binary classification, `clf.coef_` is of shape $(1,D)$, that is, `clf.coef_[1, :] - clf.coef_[0, :]`, and `clf.intercept_` is of shape $(1,)$, that is, `clf.intercept_[1] - c.intercept_[0]`.



#### `scikit-learn`'s linear discriminant analysis*

Let $X_c$ be the (centered) sample matrix for the class label $c$ , of shape $(N_c,D)$, and let $X$ be the row concatenation of $X_c$'s, of shape $(N,D)$. Denote by $\bar X$ the <u>normalized</u> sample matrix of shape $(N,D)$, i.e., each $d$-column of $X$ is divided by the standard deviation $\sigma_d$ of the samples in feature $d$.

Let $\frac{1}{\sqrt{N-C}}\bar X=U_WS_WV_W^T$ be the (reduced) singular value decomposition. The rank of $S_W$, $K_W\leq\min\{N,D\}$, and the corresponding (within-class) covariance has the form
$$
\frac{1}{N-C}\bar X^T\bar X = V_WS_W^2V_W^T
$$

Define the first projection for (normalized) within-class scatter by $W_1\equiv (V_W/\sigma_d)S_W^{-1}$ of shape $(D,K_W)$ so that $W_1^Tx=S_W^{-1}(V_W/\sigma_d)^Tx=S_W^{-1}V_W^T(x/\sigma_d)$.

Let $M$ be the (centered) mean matrix of shape $(C,D)$ with rows $\sqrt{N_c}(\mu_c-\mu)^T$ over classes, and denote by $\bar M\equiv M\cdot W_1$ the projection of the mean matrix.

Let $\frac{1}{\sqrt{C-1}}\bar M=U_BS_BV_B^T$ be the (reduced) singular value decomposition. The rank of $S_B$, $K_B\leq\min\{C-1,K_W\}\leq\min\{C-1,N,D\}$, and the corresponding (between-class) covariance has the form
$$
\frac{1}{C-1}\bar M^T\bar M = V_BS_B^2V_B^T
$$
Define the second projection for between-class scatter by $W_2\equiv V_B$ of shape $(K_W,K_B)$, and finally define the total projection by $W\equiv W_1\cdot W_2\cdot P_K = (V_W/\sigma_d)S_W^{-1}\cdot V_B\cdot P_K$ of shape $(D,K)$ where `n_components` $K\leq K_B$ and $P_K$ is the projection onto the first $K$ coordinates (corresponding to the largest $K$ singular values of $S_B$) so that
$$
W^Tx = P_K^T\cdot V_B^T\cdot S_W^{-1}\cdot V_W^T\cdot(x/\sigma_d)
$$

```python
clf = LinearDiscriminantAnalysis(solver='svd', n_components=K) # default
clf.fit(X, y)
X_trans = clf.transform(X) # dimension reduction, (X - clf.xbar_) @ clf.scalings_
clf.decision_function(X) # discriminant function w/o const (eq. 9.7)
clf.predict(X) # argmax of decision_function(X)s
clf.predict_proba(X) # softmax of decision_function(X)
    # For the binary classification: (1-p, p), where p=sigmoid(decision_function(X))
    # In this case, decision_function(X) is of shape (1,) (see below)
clf.predict_log_proba(X) # log of predict_proba(X)
```

- `clf.scalings_`: $W$ of shape `(D,K)`.
- `clf.coef_`: of shape `(C,D)`. Each row is $\beta_c=WW^T(\mu_c-\mu)$ so that $x^T\beta_c=x^TWW^T(\mu_c-\mu)=z^T(m_c-m)$ (eq. 9.7).
- `clf.intercept_`: of shape `(C,)` with values $\gamma_c$ so that $\mu_c^T\beta_c=(\mu_c-\mu)^T\beta_c+\mu^T\beta_c=\|m_c-m\|^2+\mu^T\beta_c$ (eq. 9.7). 

In binary classification, `clf.coef_` is of shape `(1,D)`, that is, `clf.coef_[1, :] - clf.coef_[0, :]`, and `clf.intercept_` is of shape `(1,)`, that is, `clf.intercept_[1] - c.intercept_[0]`.
