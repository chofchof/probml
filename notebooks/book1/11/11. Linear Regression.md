# 11. Linear Regression

The key property of the linear regression model is that the expected value of the output is assumed to be a linear function of the input, $\mathbb{E}[y|x]=w^Tx$.




## 11.2 Least squares linear regression

$$
p(y|x;\theta) = \mathcal{N}(y|w^Tx+b,\sigma^2) \tag{11.1}
$$
where $\theta=(w,b,\sigma^2)$. We will usually assume that $x=(1,x_1,\dotsc,x_D)$, so we can absorb the offset (of bias) term $b=w_0$ into the weight vector $w$.

We can always apply a nonlinear transformation to the input features, by replacing $x$ with $\phi(x)$ to get
$$
p(y|x;\theta) = \mathcal{N}(y|w^T\phi(x),\sigma^2) \tag{11.3}
$$
![](figure_11.1.png)

> Figure 11.1: Polynomial of degrees 1 and 2 fit to 21 data points.



### 11.2.2 Least squares estimation

$$
\operatorname{NLL}(w,\sigma^2) = -\sum_n\log\mathcal{N}(y_n|w^Tx_n,\sigma^2) = \frac{1}{2\sigma^2}\sum_n(y_n-\hat y_n)^2 + \frac{N}{2}\log 2\pi\sigma^2 \tag{11.5}
$$

where the predicted response $\hat y_n\equiv w^Tx_n$.

We just focus on estimating the weights $w$. In this case, the NLL is equal (up to irrelevant constants) to the **residual sum of squares**,
$$
\begin{align*}
\operatorname{RSS}(w) &\equiv \tfrac{1}{2}\sum_n (y_n-\hat y_n)^2 = \tfrac{1}{2}\|Xw-y\|_2^2 = \tfrac{1}{2}(Xw-y)^T(Xw-y) \tag{11.6} \\
\nabla_w\operatorname{RSS}(w) &= X^TXw - X^Ty \\
\nabla_w^2\operatorname{RSS}(w) &= X^TX
\end{align*}
$$



#### 11.2.2.1 Ordinary least squares

Setting $\nabla_w\operatorname{RSS}(w)=0$ and solving gives the **normal equations**, $X^TXw=X^Ty$. The corresponding solution is the **ordinary least squares (OLS)** solution,
$$
\hat w = (X^TX)^{-1}X^Ty = X^\dagger y \tag{11.9}
$$
Here, $X^\dagger\equiv (X^TX)^{-1}X^T$ is called the (left) **pseudo inverse** of $X$.

If $X$ is full rank, then the Hessian $H(w)=\nabla_w^2\operatorname{RSS}(w) = X^TX$ is positive definite so that the least squares objective has a unique global minimum at $\hat w$.

![](figure_11.2.png)

> Figure 11.2: (a) Contours of the RSS error surface for the example in Figure 11.1 (a). The blue cross represents the MLE $\hat w$. (b) Corresponding surface plot.



#### 11.2.2.2 Geometric interpretation of least squares

We will assume $N>D$. (This is known as an **overdetermined system**.)
$$
\hat y = X\hat w = XX^\dagger y = X(X^TX)^{-1}X^T y \tag{11.15}
$$
corresponds to an **orthogonal projection** of $y$ onto the column space of $X$. Here, $\operatorname{Proj}(X)\equiv XX^\dagger$ is called the **projection matrix**. In the special case that $X=x$, the orthogonal projection of $y$ onto the line $x$ becomes
$$
\operatorname{Proj}(x)y = x\Bigl(\frac{x^Ty}{x^Tx}\Bigr) \tag{11.17}
$$



#### 12.2.2.3 Algorithmic issues

Even if it is theoretically possible to compute the pseudo-inverse by inverting $X^TX$, we should not do so for numerical reasons, since $X^TX$ may be ill conditioned or singular.

1. A better (and more general) approach is to compute the pseudo-inverse using the <u>SVD</u>.

   ```python
   from scipy.linalg import lstsq
   # lstsq.default_lapack_driver = 'gelsd'
   self.coef_, _, self.rank_, self.singular_ = lstsq(X, y)
   ```

   (See LAPACK: DGELSD, https://netlib.org/lapack/explore-3.1.1-html/dgelsd.f.html)

2. If $N\gg D$, it can be quicker to use <u>QR decomposition</u>. Let $X=QR$ where $Q^TQ=I$.
   $$
   y = (QR)w \implies Q^Ty = Q^T(QR)w = Rw \implies w = R^{-1}(Q^Ty) 
   $$
   Since $R$ is upper triangular, we can solve the last equation using back-substitution, thus avoiding matrix inversion.

3. An alternative is to use <u>iterative solvers</u>, such as the **conjugate gradient** method (which assumes $X$ is symmetric positive definite), and the **GMRES** (generalized minimal residual method), that works for general $X$. These methods are well-suited to problems where $X$ is sparse or structure.

4. A final important issue is that <u>it is usually essential to standardize the input features before fitting the model</u>, to ensure that they are zero mean and unit variance.



#### 11.2.2.4 Weighted least squares

In some cases, we want to associate a weight with each sample. For example, in **heteroskedastic regression**, the variance depends on the input.
$$
p(y|x;\theta) = \mathcal{N}(y|Xw,\Lambda^{-1}) \tag{11.23}
$$
where $\Lambda=\operatorname{diag}(1/\sigma^2(x_n))$. This is known as **weighted linear regression**. In this case, the MLE is given by the **weighted least squares estimate**,
$$
\hat w = (X^T\Lambda X)^{-1}X^T\Lambda y \tag{11.24}
$$
Recall that it is called the **generalized least squares estimate** when $\Lambda$ is not diagonal.



### 11.2.3 Other approaches to computing the MLE

#### 11.2.3.1 Solving for offset and slope separately



#### 11.2.3.2 Simple linear regression (1d inputs)



#### 11.2.3.3 Partial regression



#### 11.2.3.4 Recursively computing the MLE



#### 11.2.3.5 Deriving the MLE from a generative perspective



#### 11.2.3.6 Deriving the MLE for $\sigma^2$



### 11.2.4 Measuring goodness of fit

#### 11.2.4.1 Residual plots

For 1d inputs, we can check the reasonableness of the model by plotting the residuals, $r_n=y_n-\hat y_n$ vs. the input $x_n$. The model assumes that the residuals have a $\mathcal{N}(0,\sigma^2)$ distribution, so the residual plot should be a cloud of points more or less equally above and below the horizontal line at 0, without any obvious trends.

![](figure_11.5.png)

> Figure 11.5: Residual plot for polynomial regression of degree 1 and 2 for the data in Figure 11.1. (a) We see that there is some curved structure to the residuals, indicating a lack of fit. (b) We see a much better fit.



To extend this approach to multi-dimensional inputs, we can plot $\hat y_n$ vs. the true output $y_n$, rather than plotting vs. $x_n$. A good model will have points that lie on a diagonal line.

![](figure_11.6.png)

> Figure 11.6: Fit vs. actual plots for polynomial regression of degree 1 and 2 for the data in Figure 11.1.



#### 11.2.4.2 Prediction accuracy and $R^2$

- We can assess the fit quantitatively by computing the RSS (residual sum of squares). A model with lower RSS fits the data better.

- Another measure that is used is **root mean squared error** or RMSE:

$$
\operatorname{RMSE}(w) \equiv \sqrt{\frac{1}{N}\operatorname{RSS}(w)} \tag{11.50}
$$

- A more interpretable measure can be computed using the coefficient of determination, denoted by $R^2$:
  $$
  R^2 \equiv 1- \frac{\operatorname{RSS}}{\operatorname{TSS}} = 1 - \frac{\sum_n(\hat y_n-y_n)^2}{\sum_n(\bar y_n-y_n)^2}
  $$
  where $\bar y=\frac{1}{N}\sum_n y_n$ is the empirical mean of the response and TSS is the total sum of squares.

  We see that $R^2$ measures the variance in the predictions relative to a simple constant prediction of $\hat y_n=\bar y$. Note that $0\leq R^2\leq 1$, where larger values imply a greater reduction in variance (better fit).
