# 11. Linear Regression

We will usually assume that $x$ is written as $(1,x_1,\dotsc,x_D)$, so we can absorb the offset (or bias) term $b=w_0$ into the weight vector $w$.

The <u>key property of the linear model</u> is that the expected value of the output is assumed to be a linear function of the input, i.e,
$$
\mathbb{E}[y|x] = \int y\,p(y|x;\theta)\,dy = w^Tx
$$



## 11.2 Least squares linear regression

We assume that the target variable $y$ is given by a <u>deterministic function</u> $f(x,w)$ with <u>additive Gaussian noise</u> so that $y = f(x,w) + \epsilon$, where $\epsilon\sim\mathcal{N}(0,\sigma^2)$. Thus
$$
p(y|x;\theta) = \mathcal{N}(y|f(x,w),\sigma^2) \tag{11.1}
$$
where $\theta=(w,\sigma^2)$. In this case, $\mathbb{E}[y|x]=f(x,w)$.

Set $f(x,w)=w^Tx$. If $x$ is one-dimensional, it is called **simple linear regression**. If $x$ is multi-dimensional, it is called **multiple linear regression**. If the output $y$ is also multi-dimensional, it is called **multivariate linear regression**,
$$
p(y|x;w_1,\dotsc,w_J,\sigma_1^2,\dotsc,\sigma_J^2) = \prod_j \mathcal{N}(y_j|w_j^Tx,\sigma_j^2) \tag{11.2}
$$
We can always apply a nonlinear transformation to the input features, by replacing $x$ with $\phi(x)$ to get
$$
p(y|x;\theta) = \mathcal{N}(y|w^T\phi(x),\sigma^2) \tag{11.3}
$$
![](figure_11.1.png)

> Figure 11.1: Polynomial of degrees 1 and 2 fit to 21 data points.



### 11.2.2 Least squares estimation

$$
\begin{align*}
\operatorname{NLL}(w,\sigma^2) &= -\sum_n\log\mathcal{N}(y_n|w^Tx_n,\sigma^2) \\
&= \frac{1}{2\sigma^2}\sum_n(y_n-w^Tx_n)^2 + \frac{N}{2}\log(2\pi\sigma^2) \tag{11.5}
\end{align*}
$$

We just <u>focus on</u> estimating the weights $w$. In this case, the NLL is equal (up to irrelevant constants) to the **residual sum of squares (RSS)**, which is given by
$$
\begin{align*}
\operatorname{RSS}(w) &\equiv \frac{1}{2}\sum_n (y_n-w^Tx_n)^2 \\
&= \frac{1}{2}\|Xw-y\|_2^2 = \frac{1}{2}(Xw-y)^T(Xw-y) \tag{11.6} \\
\nabla_w\operatorname{RSS}(w) &= X^TXw - X^Ty \tag{11.7} \\
\nabla_w^2\operatorname{RSS}(w) &= X^TX \tag{11.10}
\end{align*}
$$

> **IMPORTANT**: We assume that the design matrix $X$ has <u>full rank</u> and $N\geq D$.



#### 11.2.2.1 Ordinary least squares

Setting $\nabla_w\operatorname{RSS}(w)=0$ and solving gives the **normal equations**, $X^TXw=X^Ty$, since at the optimal solution, $Xw-y$ is normal (orthogonal) to the column space (range) of $X$ (see $\S$7.7.3).

The corresponding solution $\hat w$  is the **ordinary least squares (OLS)** solution, which is given by
$$
\hat w = (X^TX)^{-1}X^Ty = X^\dagger y \tag{11.9}
$$
The quantity $X^\dagger\equiv (X^TX)^{-1}X^T$ is called the **Mooreâ€“Penrose inverse** (or **pseudoinverse**) of $X$.

Moreover, the Hessian $H(w)=\nabla_w^2\operatorname{RSS}(w) = X^TX$ is <u>positive definite</u> so that the least squares objective has a <u>unique global minimum</u> at $\hat w$.

![](figure_11.2.png)

> Figure 11.2: (a) Contours of the RSS error surface for the example in Figure 11.1. The blue cross represents the MLE $\hat w$. (b) Corresponding surface plot.



#### 11.2.2.2 Geometric interpretation of least squares

We assume $N>D$ (known as an **overdetermined system**).
$$
\hat y = X\hat w = XX^\dagger y = X(X^TX)^{-1}X^T y \tag{11.15}
$$
corresponds to an **orthogonal projection** of $y$ onto the column space (range) of $X$. Here, $\operatorname{Proj}(X)\equiv XX^\dagger$ is called the **projection matrix**.

In the special case that $X=x$ is a column vector (when $D=1$), the orthogonal projection of $y$ onto the line $x$ becomes
$$
\operatorname{Proj}(x)y = x\Bigl(\frac{x^Ty}{x^Tx}\Bigr) \tag{11.17}
$$



#### 12.2.2.3 Algorithmic issues

Even if it is theoretically possible to compute the pseudoinverse, we should not do so for numerical reasons, since $X^TX$ may be <u>ill conditioned or singular</u>.

1. A better (and more general) approach is to compute the pseudoinverse using the <u>SVD</u>, $X=USV^T$ where $U$ and $V$ are orthogonal matrices. Then $X^\dagger=VS^\dagger U^T$, where $S^\dagger$ is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix (see $\S$7.5.3), and
   $$
   \hat y = XX^\dagger y = (USV^T)(VS^\dagger U^T)y = (U_KU_K^T)y
   $$
   where $K=\operatorname{rank} X$ and $U_K$ contains the first $K$ columns of $U$.

   ```python
   from scipy.linalg import lstsq
   # lstsq.default_lapack_driver = 'gelsd'
   w, residues, rank, singular_values = lstsq(X, y)
   ```

   (See LAPACK: DGELSD, https://netlib.org/lapack/explore-3.1.1-html/dgelsd.f.html)

2. If $N\gg D$, it can be quicker to use <u>QR decomposition</u>, $X=QR$ where $Q$ is an orthogonal matrix and $R=\begin{bmatrix}R_1\\0\end{bmatrix}$ with a $D\times D$ upper triangular matrix $R_1$ (see $\S$7.6.2).
   $$
   y=Xw=(QR)w \implies Q^Ty = Q^T(QR)w = Rw
   $$
   Since $R$ is upper triangular, we can solve the last equation using back-substitution, thus avoiding matrix inversion.

   ```python
   scipy.linalg.lstsq(X, y, lapack_driver='gelsy')
   ```

   (See LAPACK: DGELSY, https://netlib.org/lapack/explore-3.1.1-html/dgelsy.f.html)

3. An alternative is to use <u>iterative solvers</u>, such as the **conjugate gradient** method (which assumes $X$ is <u>symmetric positive definite</u>), and the **GMRES** (generalized minimal residual method), that works for a general <u>square</u> matrix $X$.

   These methods are well-suited to problems where $X$ is a dependency structure matrix (DSM; it is an equivalent of an adjacency matrix in graph theory) or a <u>sparse matrix</u>.

   ```python
   from scipy.sparse.linalg import cg, gmres
   w, info = cg(X, y)    # Conjugate Gradient iteration
                         # X must be symmetric positive definite
   w, info = gmres(X, y) # Generalized Minimal RESidual iteration
                         # X must be square
   ```

4. A final important issue is that <u>it is usually essential to standardize the input features before fitting the model</u>, to ensure that they are zero mean and unit variance.



#### 11.2.2.4 Weighted least squares

In some cases, we want to associate a weight with each sample. For example, in **heteroskedastic regression**, the variance depends on the input, so
$$
p(y|x;\theta) = \mathcal{N}(y|w^Tx,\sigma^2(x)) \tag{11.22}
$$
The  **weighted linear regression** has the form
$$
p(y|x;\theta) = \mathcal{N}(y|Xw,\Lambda^{-1}) \tag{11.23}
$$
where $\Lambda=\operatorname{diag}(1/\sigma^2(x_n))$. The MLE is given by the **weighted least squares (WLS) estimate**,
$$
\hat w = (X^T\Lambda X)^{-1}X^T\Lambda y \tag{11.24}
$$
Recall that it is called the **generalized least squares (GLS) estimate** if $\Lambda$ is not diagonal.



### 11.2.3 Other approaches to computing the MLE

#### 11.2.3.1 Solving for offset and slope separately

> **Reference**: [Bis06, p.142]
>
> If we make the bias parameter explicit, then the RSS (eq. 11.6) becomes
> $$
> \begin{align*}
> \operatorname{RSS}(w_0,w) &= \frac{1}{2}\sum_n(y_n-w_0-w^Tx_n)^2 = \frac{1}{2}\sum_n (y_n-w_0-\sum_{d=1:D} w_dx_{nd})^2 \\
> \frac{\partial}{\partial w_0}\operatorname{RSS}(w_0,w) &= -\sum_n (y_n-w_0-\sum_{d=1:D} w_dx_{nd}) = Nw_0 - \sum_n (y_n-\sum_{d=1:D} w_dx_{nd})
> \end{align*}
> $$
> Setting $\frac{\partial}{\partial w_0}\operatorname{RSS}(w_0,w)=0$ and solving for $w_0$ gives
> $$
> \hat w_0=\frac{1}{N}\sum_n(y_n-\sum_{d=1:D} w_dx_{nd}) = \bar y-\sum_{d=1:D} w_d\bar x_d = \bar y-w^T\bar x
> $$
> where $\bar y = \frac{1}{N}\sum_n y_n$ and $\bar x_d=\frac{1}{N}\sum_n x_{nd}$. Thus the bias $w_0$ compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the input values.

Replacing $w_0$ with $\hat w_0=\bar y-w^T\bar x$, we have
$$
\begin{align*}
\operatorname{RSS}(w) &= \frac{1}{2}\sum_n(y_n-\hat w_0-w^Tx_n)^2 = \frac{1}{2}\sum_n\bigl[(y_n-\bar y)-w^T(x_n-\bar x)\bigr]^2 \\
&= \frac{1}{2}\sum_n (y_c-w^Tx_n^c)^2 = \frac{1}{2}(X_cw-y_c)^T(X_cw-y_c)
\end{align*}
$$
where $X_c$ is the $N\times D$ <u>centered design matrix</u> containing $x_n^c=x_n-\bar x$ along it rows, and $y_c=y-\bar y$ is the <u>centered output vector</u>. We can compute $\hat w$ on centered data:
$$
\hat w = (X_c^TX_c)^{-1}X_c^Ty_c = \Bigl[\sum_n(x_n-\bar x)(x_n-\bar x)^T\Bigr]^{-1} \Bigl[\sum_n(y_n-\bar y)(x_n-\bar x)\Bigr] \tag{11.25}
$$
and then finally estimate
$$
\hat w_0=\bar y-\hat w^T\bar x \tag{11.26}
$$



#### 11.2.3.2 Simple linear regression (1d inputs)

In the case of 1d (scalar) inputs, (eq. 11.25 & 11.26) reduce to the familiar form
$$
\begin{align*}
\hat w_1 &= \frac{\sum_n(x_n-\bar x)(y_n-\bar y)}{\sum_n(x_n-\bar x)^2} = \frac{C_{xy}}{C_{xx}} \tag{11.27} \\
\hat w_0 &= \bar y-\hat w_1\bar x = \mathbb{E}[y]-\frac{C_{xy}}{C_{xx}}\mathbb{E}[x] \tag{11.28} 
\end{align*}
$$
where $C_{xy}=\operatorname{Cov}[X,Y]$ and $C_{xx}=\operatorname{Cov}[X,X]=\mathbb{V}[X]$.

> The **covariance** $\operatorname{Cov}[X,Y]\equiv\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$ between $X$ and $Y$ measures the <u>degree to which $X$ and $Y$ are (linearly) related</u>. If $X$ and $Y$ are independant, $\operatorname{Cov}[X,Y]=0$, but the converse is not true. Note that $-\infty<\operatorname{Cov}[X,Y]<\infty$.
>
> The (Pearson) **correlation coefficient** gives a normalized measure with a finite lower and upper bound $\pm1$.
> $$
> \operatorname{corr}[X,Y] \equiv \frac{\operatorname{Cov}[X,Y]}{\sqrt{\mathbb{V}[X]\mathbb{V}[Y]}} = \frac{C_{xy}}{\sqrt{C_{xx}C_{yy}}} \tag{3.7}
> $$
> Note that $\operatorname{corr}[X,Y]=1$ if and only if $Y=aX+b$ and $a>0$.

One might expect the correlation coefficient (eq. 3.7) to be related to the slope of the regression line. However, the regression coefficient is in fact given by (eq. 11.27) (see $\S$3.1.2).



#### 11.2.3.3 Partial regression

From (eq. 11.27), we can compute the **regression coefficient** of $Y=w_0+w_1X+\epsilon$ on $X$:
$$
R_{YX} \equiv \frac{\partial}{\partial x}\mathbb{E}[Y|X=x] = w_1 = \frac{C_{xy}}{C_{xx}} \tag{11.29}
$$
This is the slope of the linear prediction for $Y$ given $X$.

Consider the case of multi-dimensional inputs, so $Y=w_0+w_1X_1+\dotsc+w_DX_D+\epsilon$, where $\mathbb{E}[\epsilon]=0$. Then the optimal regression coefficient for $w_j$ is given by
$$
\begin{align*}
R_{YX_j\cdot X_1\cdots X_D} &= \frac{\partial}{\partial x}\mathbb{E}[Y|X_j=x,X_1,\dotsc,X_D] = w_j \tag{11.30} \\
&= \frac{\sum_n(x_{nj}-\bar x_j)(y_n-\bar y)}{\sum_n(x_{nj}-\bar x_j)^2}
\end{align*}
$$
which is the **partial regression coefficient** of $Y$ on $X_j$, keeping $X_i$ constant for all $i\neq j$. This quantity is invariant to the specific values of $X_i$'s ($i\neq j$) we condition on. This means that we can interpret the $j$'th coefficient $\hat w_j$ as the change in output $y$ we expect per unit change in input $x_j$, keeping all the other inputs constant. 



#### 11.2.3.4 Recursively computing the MLE

OLS is a <u>batch method</u> for computing the MLE (see eq. 11.27 and 11.28). In some applications, the data arrives in a continual stream, so we want to compute the estimate online, or recursively (see $\S$4.4.2).

Suppose $Y=w_0+w_1X+\epsilon$, simple 1d linear regression. Define
$$
\begin{align*}
\bar x^{(n)} &= \frac{1}{n}\sum_{i=1:n} x_i, \quad \bar y^{(n)} = \frac{1}{n}\sum_{i=1:n} y_i \\
C_{xx}^{(n)} &= \frac{1}{n}\sum_{i=1:n} (x_i-\bar x^{(n)})^2, \quad
C_{yy}^{(n)} = \frac{1}{n}\sum_{i=1:n} (y_i-\bar y^{(n)})^2, \\
C_{xy}^{(n)} &= \frac{1}{n}\sum_{i=1:n} (x_i-\bar x^{(n)})(y_i-\bar y^{(n)}) \tag{11.34}
\end{align*}
$$

1. We can update the means online using
$$
  \bar x^{(n+1)} = \bar x^{(n)} + \frac{1}{n+1}(x_{n+1}-\bar x^{(n)}), \quad
  \bar y^{(n+1)} = \bar y^{(n)} + \frac{1}{n+1}(y_{n+1}-\bar y^{(n)})
$$

2. To update the covariance terms, let us first rewrite $C_{xy}^{(n)}$ as follows:
$$
\begin{align*}
C_{xy}^{(n)} &= \frac{1}{n} \Bigl[ \sum_{i=1:n} x_iy_i - n\bar x^{(n)}\bar y^{(n)} \Bigr] \implies
  \sum_{i=1:n} x_iy_i = nC_{xy}^{(n)}+n\bar x^{(n)}\bar y^{(n)} \\
  C_{xy}^{(n+1)} &= \frac{1}{n+1} \Bigl[ x_{n+1}y_{n+1} +   nC_{xy}^{(n)} + n\bar x^{(n)}\bar y^{(n)} - (n+1)\bar x^{(n+1)}\bar y^{(n+1)} \Bigr] \tag{11.40}
\end{align*}
$$
3. From (eq. 11.27 and 11.28) we have

$$
w_1^{(n+1)} = \frac{C_{xy}^{(n+1)}}{C_{xx}^{(n+1)}} \quad\text{and}\quad
  w_0^{(n+1)} = \bar y^{(n+1)} - w_1^{(n+1)} \bar x^{(n+1)}
$$


![](figure_11.4.png)

> Figure 11.4: Regression coefficients over time for the 1d model in Figure 11.1.



To extend the above analysis to multi-dimensional inputs, the easiest approach is to use SGD with a minibatch size of 1.
$$
w_{t+1} = w_t - \eta_t(w_t^Tx_n-y_n)x_n \tag{8.60}
$$
where $n=n(t)$ is the index of the sample chosen at iteration $t$. The overall algorithm is called the **least mean squares (LMS)** algorithm (see $\S$8.4.2).



#### 11.2.3.5 Deriving the MLE from a generative perspective

Linear regression is a discriminative model of the form $p(y|x)$. However, we can also use <u>generative model for regression</u>. The goal is to compute the conditional expectation
$$
f(x) = \mathbb{E}[y|x] = \int y\,p(y|x)\,dy = \frac{\int y\,p(x,y)\,dy}{\int p(x,y)\,dy} \tag{11.41}
$$
Suppose we fit $p(x,y)$ using an <u>MVN (multivariate Gaussian)</u>. The MLEs for the parameters of the joint distribution are the <u>empirical mean and covariances</u>,
$$
\begin{align*}
\mu_x &= \frac{1}{N}\sum_n x_n = \bar x \quad \mu_y = \frac{1}{N}\sum_n y_n = \bar y \\
\Sigma_{xx} &= \frac{1}{N}\sum_n (x_n-\bar x)(x_n-\bar x)^T = \frac{1}{N}X_c^TX_c \\
\Sigma_{xy} &= \frac{1}{N}\sum_n (x_n-\bar x)(y_n-\bar y)^T = \frac{1}{N}X_c^Ty_c
\end{align*}
$$
Hence from (eq. 3.28) we have $\mathbb{E}[y|x] = \mu_y+\Sigma_{xy}^T\Sigma_{xx}^{-1}(x-\mu_x)$. We can rewrite this as $\mathbb{E}[y|x]=w_0+w^Tx$ by defining
$$
\begin{align*}
w &= \Sigma_{xx}^{-1}\Sigma_{xy} = (X_c^TX_c)^{-1}X_c^Ty_c \tag{11.48} \\
w_0 &= \mu_y - \Sigma_{xy}^T\Sigma_{xx}^{-1}\mu_x = \mu_y-w^T\mu_x = \bar y-w^T\bar x \tag{11.47}
\end{align*}
$$
This matches the MLEs for the discriminative model as (eq. 11.25 and 11.26).

Thus we see that fitting the joint model, and then conditioning it, <u>yields the same result</u> as fitting the conditional model. However, <u>this is only true for Gaussian models</u>.



#### 11.2.3.6 Deriving the MLE for $\sigma^2$

After estimating $\hat w$, we can estimate the noise variance. From (eq 11.5) we have
$$
\hat\sigma^2 = \arg\min_{\sigma^2}\operatorname{NLL}(\hat w,\sigma^2) = \frac{1}{N}\sum_n(y_n-\hat w^Tx_n)^2
$$



### 11.2.4 Measuring goodness of fit

#### 11.2.4.1 Residual plots

For 1d inputs, we can check the reasonableness of the model by plotting the residuals, $r_n=y_n-\hat y_n$ vs. the input $x_n$. The model assumes that the residuals have a $\mathcal{N}(0,\sigma^2)$ distribution, so the **residual plot** should be a cloud of points more or less equally above and below the horizontal line at 0, <u>without any obvious trends</u>.

![](figure_11.5.png)

> Figure 11.5: Residual plot for polynomial regression of degree 1 and 2 for the data in Figure 11.1. (a) We see that there is some curved structure to the residuals, indicating a lack of fit. (b) We see a much better fit.



To extend this approach to multi-dimensional inputs, we can plot $\hat y_n$ vs. the true output $y_n$, rather than plotting vs. $x_n$. A good model will have points that lie on a <u>diagonal line</u>.

![](figure_11.6.png)

> Figure 11.6: Fit vs. actual plots for polynomial regression of degree 1 and 2 for the data in Figure 11.1.



#### 11.2.4.2 Prediction accuracy and $R^2$

- We can assess the fit quantitatively by computing the RSS (residual sum of squares). A model with lower RSS fits the data better.

- Another measure that is used is **root mean squared error (RMSE)**:

$$
\operatorname{RMSE}(w) \equiv \sqrt{\frac{1}{N}\operatorname{RSS}(w)} \tag{11.50}
$$

- A more interpretable measure can be computed using the **coefficient of determination**, denoted by $R^2$:
  $$
  R^2 \equiv 1- \frac{\operatorname{RSS}}{\operatorname{TSS}} = 1 - \frac{\sum_n(\hat y_n-y_n)^2}{\sum_n(\bar y_n-y_n)^2}
  $$
  where $\bar y=\frac{1}{N}\sum_n y_n$ is the empirical mean of the response and TSS is the total sum of squares.

  We see that $R^2$ measures the variance in the predictions relative to a simple constant prediction of $\hat y_n=\bar y$. Note that $0\leq R^2\leq 1$, where larger values imply a greater reduction in variance (better fit).



## 11.3 Ridge regression

MLE can result in overfitting. A simple solution it to use MAP estimation with a zero-mean Gaussian prior, $p(w)=\mathcal{N}(w|0,\tau^2 I)$. This is called **ridge regression**.
$$
\begin{align*}
\hat w_\text{map} &= \arg\min_w \frac{1}{2\sigma^2}(y-Xw)^T(y-Xw)+\frac{1}{2\tau^2}w^Tw \\
&= \arg\min_w \operatorname{RSS}(w) + \frac{\lambda}{2} \|w\|_2^2 \tag{11.53}
\end{align*}
$$
where $\lambda\equiv\sigma^2/\tau^2$ is proportional to the strength of the prior, and $\|w\|_2^2=\sum_{d=1:D}|w_d|^2$ (without the offset term $w_0$). In general, this technique is called $\ell_2$ **regularization** or **weight decay**.

> Note that we <u>do not penalize</u> $w_0$, since that only affects the global mean of the output, and does not contribute to overfitting.
>
> **Exercise**: Show that the optimizer of
> $$
> J(w,w_0) = (y-Xw-w_01_N)^T(y-Xw-w_01_N) + \lambda (w^Tw+w_0^2)
> $$
> is $\hat w_0=\frac{N}{N+\lambda}(\bar y-w^T\bar x)$ and $\hat w=(X_c^TX_c+\lambda I_D)^{-1}X_c^Ty_c$, where $X_c$ contains $x_n^c=x_n-\frac{N}{N+\lambda}\bar x$ along it rows, and $y_c=y-\frac{N}{N+\lambda}\bar y$.

> **Reference**: [Bis06, p.10] Note that often the coefficient $w_0$ is <u>omitted</u> from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable (Hastie et al., 2001), or it may be <u>included but with its own regularization coefficient</u> (we shall discuss this topic in more detail in $\S$5.5.1).
>
> [Bis06, p.259] It is therefore common to include <u>separate priors for the biases</u> (which then break shift invariance) having their own hyperparameters.
>
> *Is pytorch SGD optimizer apply weight decay to bias parameters with default settings?* (YES)
>
> https://github.com/pytorch/pytorch/issues/2639
>



### 11.3.1 Computing the MAP estimate

From (eq. 11.53) the MAP estimate corresponds to minimizing
$$
J(w) \equiv (y-Xw)^T(y-Xw) + \lambda\|w\|_2^2 \tag{11.55}
$$
where $\lambda=\sigma^2/\tau^2$ is the strength of the regularizer. The derivative is given by
$$
\nabla_w J(w) = 2 (X^TXw-X^Ty+\lambda w) \tag{11.56}
$$
and hence
$$
\hat w_\text{map} = (X^TX+\lambda I_D)^{-1}X^Ty = \bigl(\sum_n x_nx_n^T + \lambda I_D\bigr)^{-1}\sum_n y_nx_n \tag{11.57}
$$



#### 11.3.1.1 Solving using QR

Naively computing the primal estimate $\hat w_\text{map}$ using matrix inversion is a bad idea, since it can be slow and numerically unstable.

We assume the prior is $p(w)=\mathcal{N}(0,\Lambda^{-1})$, where $\Lambda$ is the precision matrix. In the case of ridge regression, $\Lambda=\tau^{-2}I$.

Let $\tilde X=\begin{pmatrix}X/\sigma \\ \sqrt{\Lambda}\end{pmatrix}$ and $\tilde y=\begin{pmatrix}y/\sigma \\ 0_D\end{pmatrix}$ where $\Lambda=\sqrt{\Lambda}\sqrt{\Lambda}^T$ is a <u>Cholesky decomposition</u> of $\Lambda$. Then the RSS on this expanded data is equivalent to <u>penalized</u> RSS on the original data:
$$
\begin{align*}
f(w) &= (\tilde y-\tilde Xw)^T(\tilde y-\tilde Xw)
= \begin{pmatrix}\frac{1}{\sigma}(y-Xw)\\-\sqrt{\Lambda}w\end{pmatrix}^T \begin{pmatrix}\frac{1}{\sigma}(y-Xw)\\-\sqrt{\Lambda}w\end{pmatrix} \\
&= \frac{1}{\sigma^2}(y-Xw)^T(y-Xw)+w^T\Lambda w \tag{11.63}
\end{align*}
$$
In the case of ridge regression, $f(w)=\frac{1}{\sigma^2}J(w)$. Hence the MAP estimate can be given by
$$
\hat w_\text{map}=(\tilde X^T\tilde X)^{-1}\tilde X^T\tilde y \tag{11.64}
$$
which is solved using standard OLS methods.

In particular, we can compute the QR decomposition of the $(N+D)\times D$ matrix $\tilde X$, and then proceed as in $\S$11.2.2.3. This takes $O((N+D)D^2)$ time. (https://math.stackexchange.com/questions/501018/what-is-the-operation-count-for-qr-factorization-using-householder-transformatio)



#### 11.3.1.2 Solving using SVD

We assume $D\gg N$ (usual in ridge regression). In this case, it is faster to use SVD than QR.

Let $X=USV^T$ be the SVD of $X$, where $U$ is $N\times N$ orthogonal, $S=\operatorname{diag}(\sigma_1,\dotsc,\sigma_N)$, and $V$ is a $D\times N$ matrix with orthonormal columns such that $V^TV=I_N$.

> **Reference**: [HTF09, $\S$18.3.5]
>
> Let $R\equiv US$. Then $X=RV^T$ and we have
> $$
> \begin{align*}
> \hat w_\text{map} &= (X^TX+\lambda I_D)^{-1}X^Ty = (VR^TRV^T+\lambda I_D)^{-1}VR^Ty \\
> &= V(R^TR+\lambda I_N)^{-1}R^Ty \tag{11.65}
> \end{align*}
> $$
> since $(VR^TRV^T+\lambda I_D)V=VR^TR+\lambda V=V(R^TR+\lambda I_N)$.

In other words, we can replace the $D$-dimensional vectors $x_n$ (rows of $X$) with the $N$-dimensional vectors $r_n$ (rows of $R$) and perform our penalized fit as before. The overall time is $O(DN^2)$ operations, which is less than $O(D^3)$ if $D\gg N$.



### 11.3.2 Connection between ridge regression and PCA

> **Reference**: [HFT09, p.66]
>
> From (eq. 11.65) the ridge predictions on the training set are given by
> $$
> \begin{align*}
> X\hat w_\text{map} &= (USV^T)V(S^2+\lambda I_N)^{-1}SU^Ty
> = \sum_n \Bigl(\frac{\sigma_n^2}{\sigma_n^2+\lambda}\Bigr)u_nu_n^Ty \tag{11.69} \\
> X\hat w_\text{mle} &= (USV^T)V(S^2)^{-1}SU^Ty = UU^Ty = \sum_n u_nu_n^Ty \tag{11.70}
> \end{align*}
> $$
> where the $u_n$ are the columns of $U$. Note that since $\lambda\geq 0$, we have $0\leq\frac{\sigma_n^2}{\sigma_n^2+\lambda}\leq 1$.

If $\sigma_n^2$ is small compared to $\lambda$, then the direction $u_n$ will not have much effect on the prediction. In view of this, we define the effective number of **degrees of freedom** of the model
$$
\operatorname{dof}(\lambda) = \sum_n \frac{\sigma_n^2}{\sigma_n^2+\lambda} \tag{11.71}
$$
Note that $\operatorname{dof}(0)=N$ and $\operatorname{dof}(\lambda)\to0$ as $\lambda\to\infty$.



**Question**: Why is this behavior desirable?





## 11.7 Bayesian linear regression *

We discuss how to compute the posterior over the parameters, $p(\theta|\mathcal{D})$. For simplicity, we assume the variance is known, so we just want to compute $p(w|\mathcal{D},\sigma^2)$.



### 11.7.1 Priors

For simplicity, we will use a Gaussian prior, $p(w)=\mathcal{N}(w|\breve w,\breve\Sigma)$. This is a small generalization of the prior $\mathcal{N}(w|0,\tau^2I)$ in ridge regression.



### 11.7.2 Posteriors

$$
p(\mathcal{D}|w,\sigma^2) = \prod_n p(y_n|w^Tx,\sigma^2) = \mathcal{N}(y|Xw,\sigma^2I_N) \tag{11.119}
$$

We can use Bayes rule for Gaussians (eq. 3.37) to derive the posterior
$$
p(w|X,y,\sigma^2) \propto \mathcal{N}(y|Xw,\sigma^2I_N) \mathcal{N}(w|\breve w,\breve\Sigma) \equiv \mathcal{N}(w|\bar w,\bar\Sigma)
$$
where $\bar w\equiv\bar\Sigma \bigl( \frac{1}{\sigma^2}X^Ty+\breve\Sigma^{-1}\breve w \bigr)$ is the posterior mean and $\bar\Sigma^{-1}\equiv\breve\Sigma^{-1}+\frac{1}{\sigma^2}X^TX$ is the posterior covariance.

In ridge regression where $\breve w=0$, $\breve\Sigma=\tau^2I$, and $\lambda=\sigma^2/\tau^2$, the posterior mean becomes the MAP estimate (eq. 11.57), since
$$
\begin{gather*}
\bar\Sigma^{-1} = \frac{1}{\tau^2}I + \frac{1}{\sigma^2}X^TX = \frac{1}{\sigma^2}(X^TX+\lambda I) \\
\bar w = \sigma^2(X^TX+\lambda I)^{-1}\Bigl(\frac{1}{\sigma^2}X^Ty+0\Bigr)=(X^TX+\lambda I)^{-1}X^Ty = \hat w_\text{map}
\end{gather*}
$$
